# -*- coding: utf-8 -*-
"""A2_zili6245_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p4IdqD81p9oEssiKs-TkY-qRUO-Nyz0F
"""


"""# Load Json Data"""

import random
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.types import StructField, StructType, StringType, IntegerType
from math import ceil

WINDOW_SIZE = 4096
STRIDE = 2048

spark = SparkSession \
    .builder \
    .appName("Comp5349 assignment2_luna") \
    .getOrCreate()

train_data = "s3://comp5349-2022/CUADv1.json"
test_data = "s3://comp5349-2022/test.json"

data_init_df = spark.read.option("multiline","true").json(test_data)

print('\n'+ ('*'*50))
print('Data schema of the input json file')
print(data_init_df.printSchema())
print(('*'*50) + '\n')

data_df= data_init_df.select((explode("data").alias('data')))

print('\n'+ ('*'*50))
print('This file has {:3d} contracts'.format(data_df.count()))
print(('*'*50) + '\n')

"""# Create RDD"""

def rowToTuple(row):
  row = row.asDict()
  # ['title': string, 'paragraphs': list(row)]
  contract = row['data'].asDict()
  return (contract['title'], contract['paragraphs'])

data_rdd = data_df.rdd.map(rowToTuple)

def contractToQas(row):
  # ['context': string, 'qas': list(row)]
  context_dict = row[1][0].asDict()
  return [((row[0], context_dict['context']), qas) for qas in context_dict['qas']]

# ((title, context), qas) 41 categories in each contract
qas_rdd = data_rdd.flatMap(contractToQas)

print('\n'+ ('*'*50))
print('There are total {:3d} question contract pairs for all contracts.'.format(qas_rdd.count()))
print(('*'*50) + '\n')

def contextToQas(row):
  # ['answers': list(row), 'id': string, 'is_impossible': boolean, 'question': string]
  qas_dict = row[1].asDict()
  return (qas_dict['is_impossible'], (row[0], (qas_dict['question'], qas_dict['answers'])))

# ((title, context), (question, answers_list)) for each category
impossible_negative_rdd = qas_rdd.map(contextToQas).filter(lambda rec: rec[0] == True).values()
possible_rdd = qas_rdd.map(contextToQas).filter(lambda rec: rec[0] == False).values()

print('\n'+ ('*'*50))
print('There are total {:3d} possitive question contract pairs for all contracts.'.format(possible_rdd.count()))
print('There are total {:3d} impossible question contract pairs for all contracts.'.format(impossible_negative_rdd.count()))
print(('*'*50) + '\n')

"""## Possible RDD"""

def queToAnswerKV(row):
  title = row[0][0]
  context = row[0][1]
  que = row[1][0]
  answer_list = row[1][1]
  return [((title, context), (que, answer)) for answer in answer_list]

# ((title, context), (question, answer))
possible_KV_rdd = possible_rdd.flatMap(queToAnswerKV)

print('\n'+ ('*'*50))
print('Flatten answer_list of each question and there are {:3d} possible question answer pairs'.format(possible_KV_rdd.count()))
print(('*'*50) + '\n')

def caculateEnd(row):
  """
  Calculate end based on the answer and start
  """
  title = row[0][0]
  context = row[0][1]
  que = row[1][0]
  answer_dict = row[1][1].asDict()

  # caculate end location
  text = answer_dict['text']
  start = answer_dict['answer_start']
  end = start + len(text)
  return ((title, context), (que, (text, (start, end))))

# ((title, context), (question, (text, (start, end))))
possible_loc_rdd = possible_KV_rdd.map(caculateEnd)

print('\n'+ ('*'*50))
print('{:3d} questions are covered in positive samples for all contracts'.format(possible_loc_rdd.values().keys().distinct().count()))
print(('*'*50) + '\n')

def generateSeq(row):
  """
  func: Generate sequences, seq_loc, flag; update source start and source end
  source: contract sequences
  seq_start: start location of each sequence
  flag: used to identify positive and negative
  """
  title = row[0][0]
  context = row[0][1]
  que = row[1][0]
  text = row[1][1][0]
  start = row[1][1][1][0]
  end = row[1][1][1][1]

  context_length = len(context)
  seq_list = []
  i = 0
  for num in range(ceil(context_length/STRIDE)):
    # generate sequences
    if num < (ceil(context_length/STRIDE)-1):
      seq = context[i:i+WINDOW_SIZE]
    else:
      seq = context[i:context_length]

    # generate flag information
    s_start, s_end = 0, 0
    if (i <= start-len(seq)):
      flag = 'negative'
    elif (i >=end):
      flag = 'negative'
    else:
      flag = 'positive'
      # update start and end
      s_start = ((start - i) if i < start else 0)
      s_end = ((end - i) if i+len(seq) > end else len(seq))

    seq_dict = {'flag': flag, 'source': seq, 'seq_start': i, 'start': s_start, 'end': s_end}
    seq_list.append(seq_dict)
    i += STRIDE
  return [(sd['flag'], ((title, sd['source']), (sd['seq_start'], (que, (sd['start'], sd['end']))))) for sd in seq_list]

possible_sample_rdd = possible_loc_rdd.flatMap(generateSeq)

# ((title, source), (seq_start, (question, (start, end))))
positive_rdd = possible_sample_rdd.filter(lambda rec: rec[0] == 'positive').values()
possible_negative_rdd = possible_sample_rdd.filter(lambda rec: rec[0] == 'negative').values()

print('\n'+ ('*'*50))
print('There are total {:3d} possible positive sequences.'.format(positive_rdd.count()))
print('There are total {:3d} possible negative sequences.'.format(possible_negative_rdd.count()))
print(('*'*50) + '\n')

"""## Positive Samples

### Generate Positive Samples
"""

# (seq, ques, start, end)
pos_sample = positive_rdd.map(lambda row: (row[0][1], row[1][1][0], row[1][1][1][0], row[1][1][1][1]))

exm = pos_sample.take(1)[0]
print('\n'+ ('*'*50))
print("Possible positive examples:")
print('question: '+exm[1] + '; start: '+str(exm[2])+'; end: '+str(exm[3]))
print(('*'*50) + '\n')

"""### Positive Coverage"""

def setGenerator(v1, v2):
  """
  Generate the location set of the sequences that each contract covered
  """
  if v1 == None:
    v1 = set(v2)
  else:
    v1.add(v2)
  return v1

# (title, seq_start_list)
positive_coverage_rdd = positive_rdd\
                        .map(lambda row: (row[0][0], row[1][0]))\
                        .aggregateByKey(set(), setGenerator, setGenerator)

# positive coverage dictionary used for maximum coverage
# {title: seq_start_list}
positive_coverage_dict = positive_coverage_rdd.collectAsMap()

# positive_coverage_dict samples
title = random.sample(positive_coverage_dict.keys(), 1)[0]
loc_list = positive_coverage_dict[title]
print('\n'+ ('*'*50))
print("Positive coverage example:")
print(title+' --> ')
print(loc_list)
print(('*'*50) + '\n')

"""## Possible Negative Samples

### Number of Possible Negative
"""

# positive_rdd = ((title, source), (seq_start, (question, (start, end))))
# the number of positive samples for each question in each contract
# ((ques, title), pos_num)
pos_num_rdd = positive_rdd\
              .map(lambda row: ((row[1][1][0], row[0][0]), row[1][0]))\
              .aggregateByKey(0, lambda u, v: u + 1, lambda u1, u2: u1 + u2)

def posQuesDict_sameP(u, v):
  """
  Create {ques: seq_loc} for each (contract, source) in same partition
  """
  ques, pos_num = v[0], v[1]
  u[ques] = pos_num
  return u

def posQuesDict_diffP(u1, u2):
  """
  Create {ques: seq_loc} for each (contract, source) across different partitions
  """
  u = {}
  if len(u1) != 0:
    u = u1
    u.update(u2)
  else:
    u = u2
  return u

# (title, posQues_dict)
pos_que_num_rdd = pos_num_rdd\
                  .map(lambda row: (row[0][1], (row[0][0], row[1])))\
                  .aggregateByKey({}, posQuesDict_sameP, posQuesDict_diffP)

# used for generate possible negative samples
# {title: {ques: pos_num}}
possible_neg_dict = pos_que_num_rdd.collectAsMap()

"""### Generate Possible Negative Samples"""

def seqLocDict_sameP(u, v):
  """
  Create {seq_loc: source} in same partition
  """
  loc, seq = v[0], v[1]
  u[loc] = seq
  return u

def seqLocDict_diffP(u1, u2):
  """
  Create {seq_loc: source} across different partitions
  """
  u = {}
  if len(u1) != 0:
    u = u1
    u.update(u2)
  else:
    u = u2
  return u

# possible_negative_rdd = ((title, source), (seq_start, (question, (start, end))))
# ((title, ques), seq_dict)
possible_neg_rdd = possible_negative_rdd\
                   .map(lambda row: ((row[0][0], row[1][1][0]), (row[1][0], row[0][1])))\
                   .aggregateByKey({}, seqLocDict_sameP, seqLocDict_diffP)

# positive_coverage_dict = {title: seq_start_list}
# possible_neg_dict = {title: {ques: pos_num}}
def selectPosNeg(row):
  """
  Generate possible negative samples
  """
  title, ques, seq_dict = row[0][0], row[0][1], row[1]
  seq_keys = set(seq_dict.keys())
  seqloc_list = []
  pos_num = possible_neg_dict[title][ques]
  # positive coverage
  pos_cover = positive_coverage_dict[title]
  # ensure maximum coverage
  unique_set = seq_keys - pos_cover
  unique_list = sorted(list(unique_set))
  if len(unique_list) > pos_num:
    # following primary heuristics
    seqloc_list = unique_list[-pos_num:]
  else:
    # seqloc_list = unique_list
    if len(seq_keys) > pos_num:
      # if the number of sequences bigger than the suggested number
      random_num = pos_num - len(unique_list)
      # use random number of samples from covered sequences
      random_keys = random.sample(list(seq_keys - unique_set), random_num)
      seqloc_list = random_keys + unique_list
    else:
      seqloc_list = list(seq_keys)
  # generate seq_list based on seqloc_list
  seq_list = []
  for loc in seqloc_list:
    if seq_list == None:
      seq_list = seq_dict[loc]
    else:
      seq_list.append(seq_dict[loc])
  return [(seq, ques, 0, 0)for seq in seq_list]

# (seq, ques, start, end)
pos_neg_sample = possible_neg_rdd.flatMap(selectPosNeg)

exm = pos_neg_sample.take(1)[0]
print('\n'+ ('*'*50))
print('{:3d} possible negative samples are remained.'.format(pos_neg_sample.count()))
print("Possible negative examples:")
print('question: '+exm[1] + '; start: '+str(exm[2])+'; end: '+str(exm[3]))
print(('*'*50) + '\n')

"""## Impossible Negative Samples

### Number of Impossible Negative
"""

# positive_rdd = ((title, source), (seq_start, (question, (start, end))))
# (ques, ((total_posNum, contractInQuesNum)))
# contractInQuesNum: contracts that have at least one positive sample for this question
im_number_rdd = pos_num_rdd\
                .map(lambda row: (row[0][0], (row[0][1], row[1])))\
                .aggregateByKey((0,0), lambda u, v: (u[0] + v[1], u[1] + 1), lambda u1, u2: (u1[0] + u2[0], u1[1] + u2[1]))

# the average number of positive samples of that question in other contracts that have at least one positive sample for the same question
im_quenum_dict = im_number_rdd.collectAsMap()
for num in im_quenum_dict:
  pos_num, con_num = im_quenum_dict[num][0], im_quenum_dict[num][1]
  avg = round(pos_num/con_num)
  # {ques: avg_num}
  im_quenum_dict[num] = avg

# im_quenum_dict samples
ques = random.sample(im_quenum_dict.keys(), 1)[0]
avg = im_quenum_dict[ques]
print('\n'+ ('*'*50))
print("Positive coverage example:")
print(ques)
print('The average positive number of this question is: '+str(avg))
print(('*'*50) + '\n')

"""### Generate Impossible Negative Samples"""

def imNegSample(row):
  """
  Generate seq_dict for sample generation
  seq_dict: {seq_loc: source}
  """
  title, context, ques = row[0][0], row[0][1], row[1][0]
  context = row[0][1]
  context_length = len(context)
  seq_dict = {}
  i = 0
  for num in range(ceil(context_length/STRIDE)):
    if num < (ceil(context_length/STRIDE)-1):
      seq_dict[i] = context[i:i+WINDOW_SIZE]
    else:
      seq_dict[i] = context[i:context_length]
    i += STRIDE
  return ((title, ques), seq_dict)

# ((title, ques), seq_dict)
im_neg_rdd = impossible_negative_rdd.map(imNegSample)

# number of questions that have impossible negative
print('\n'+ ('*'*50))
print('{:3d} questions are covered in impossible samples for all contracts'.format(im_neg_rdd.keys().values().distinct().count()))
print(('*'*50) + '\n')

# positive_coverage_dict = {title: seqloc_list}
# im_quenum_dict = {ques: avg_num}
def selectImNeg(row):
  """
  Generate impossible negative samples
  """
  title, ques, seq_dict = row[0][0], row[0][1], row[1]
  seq_keys = set(seq_dict.keys())
  ques_set = set(im_quenum_dict.keys())
  seqloc_list = []
  # check is the question has positive answer
  if ques in ques_set:
    # positive coverage
    pos_cover = positive_coverage_dict[title]
    # suggest number of impossible negative
    im_num = im_quenum_dict[ques]
    # ensure maximum coverage
    unique_list = sorted(list(seq_keys - pos_cover))
    if len(unique_list) > im_num:
      # following primary heuristics
      seqloc_list = unique_list[0:im_num]
    else:
      # seqloc_list = unique_list
      if len(seq_keys) > im_num:
        # if the number of sequences bigger than the suggested number
        random_num = im_num - len(unique_list)
        # use random number of samples from covered sequences
        random_keys = random.sample(list(pos_cover), random_num)
        seqloc_list = random_keys + unique_list
      else:
        seqloc_list = list(seq_keys)
  else:
    # if there is a question that have no answers
    seqloc_list = list(seq_keys)
  # generate seq_list based on seqloc_list
  seq_list = []
  for loc in seqloc_list:
    if seq_list == None:
      seq_list = seq_dict[loc]
    else:
      seq_list.append(seq_dict[loc])
  return [(seq, ques, 0, 0)for seq in seq_list]

# (seq, ques, start, end)
im_neg_sample = im_neg_rdd.flatMap(selectImNeg)

exm = im_neg_sample.take(1)[0]
print('\n'+ ('*'*50))
print('{:3d} impossible negative samples are remained.'.format(im_neg_sample.count()))
print("Impossible negative examples:")
print('question: '+exm[1] + '; start: '+str(exm[2])+'; end: '+str(exm[3]))
print(('*'*50) + '\n')

"""# Final Sample and Json Output"""

# (seq, ques, start, end) for all positive and negative
sample_rdd = pos_sample.union(pos_neg_sample.union(im_neg_sample))

print('\n'+ ('*'*50))
print('{:3d} samples are generated.'.format(sample_rdd.count()))
print(('*'*50) + '\n')

# schema
schema = StructType([
        StructField("source", StringType(), True),
        StructField("question", StringType(), True),
        StructField("answer_start", IntegerType(), True),
        StructField("answer_end", IntegerType(), True)
    ])
sample_df = spark.createDataFrame(sample_rdd, schema=schema)

print('\n'+ ('*'*50))
print('Data schema of the output json file')
print(sample_df.printSchema())
print(('*'*50) + '\n')

filepath = '～/'
sample_df.coalesce(1).write.format('json').save(filepath)
