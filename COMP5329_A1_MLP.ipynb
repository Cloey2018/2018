{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_128.h5: (60000, 128)\n",
      "train_label.h5: (60000,)\n",
      "test_128.h5: (10000, 128)\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# IMPORT LIBRARIES\n",
    "# =================\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Read all data and load into files\n",
    "with h5py.File('/Users/liwenxuan/Desktop/Assignment-1-Dataset/train_128.h5', 'r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "\n",
    "with h5py.File('/Users/liwenxuan/Desktop/Assignment-1-Dataset/train_label.h5', 'r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "with h5py.File('/Users/liwenxuan/Desktop/Assignment-1-Dataset/test_128.h5', 'r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "\n",
    "print('train_128.h5: ' + str(data.shape))\n",
    "print('train_label.h5: ' + str(label.shape))\n",
    "print('test_128.h5: ' + str(test_data.shape))\n",
    "print(\"\\n----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to array\n",
    "input_data = np.array(data, dtype=float)\n",
    "test_data = np.array(test_data, dtype=float)\n",
    "\n",
    "# Normalization Data (x-mu)/delta\n",
    "input_data = (input_data - input_data.mean(axis=0, keepdims=True)) / input_data.std(axis=0, keepdims=True)\n",
    "test_data = (test_data - test_data.mean(axis=0, keepdims=True)) / test_data.std(axis=0, keepdims=True)\n",
    "\n",
    "# Transform to one-hot label\n",
    "label_data = np.array(np.eye(10)[label.reshape(-1)], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)\n",
    "        return 1.0 - a**2\n",
    "\n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_derivative(self, a):\n",
    "        # a = logistic(x)\n",
    "        return a * (1 - a)\n",
    "\n",
    "    # define relu activation and relu derivative\n",
    "    def __relu(self, x):\n",
    "        index = (x < 0)\n",
    "        x[index] = 0\n",
    "        return x\n",
    "\n",
    "    def __relu_deriv(self, a):\n",
    "        index1 = (a>=0)\n",
    "        index0 = np.logical_not(index1)\n",
    "        a[index1]=1\n",
    "        a[index0]=0\n",
    "        return a\n",
    "\n",
    "    # default activation is relu\n",
    "    def __init__(self, activation='relu'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "\n",
    "        # choose relu\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputOuput_Layer(object):\n",
    "    def __init__(self, n_in, n_out, W=None, b=None, activation='relu', weight_norm=None):\n",
    "        self.input = None\n",
    "        self.activation = Activation(activation).f\n",
    "        self.activation_deriv = Activation(activation).f_deriv\n",
    "\n",
    "        # so use He initialization\n",
    "        self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n",
    "        self.b = np.zeros(n_out,)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        # set whether implement weight normalization\n",
    "        self.weight_norm = weight_norm\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "\n",
    "        # batch normalization init gamma and beta\n",
    "        self.gamma_BN = np.ones((1, n_in))\n",
    "        self.beta_BN = 0 #np.zeros(n_in,)\n",
    "        self.grad_gamma_BN = np.ones(self.gamma_BN.shape)\n",
    "        self.grad_beta_BN = 0 #np.zeros(self.beta_BN.shape)\n",
    "\n",
    "        # momentum init v and b params\n",
    "        self.v_w = np.zeros(self.W.shape)\n",
    "        self.v_b = np.zeros(self.b.shape)\n",
    "        # momentum with batch normalization init\n",
    "        self.v_gamma_BN = np.zeros(self.gamma_BN.shape)\n",
    "        self.v_beta_BN = 0\n",
    "\n",
    "    def forward(self, input, BN=False, err_BN=0):\n",
    "        # judge whether weight normalization and implement\n",
    "        if self.weight_norm is True:\n",
    "            self.W = self.W / (((self.W ** 2).sum(axis = 0, keepdims = True)) ** 0.5)\n",
    "            self.W = np.nan_to_num(self.W)\n",
    "        # implement forward function and judge whether use activation function\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    def forward_predict(self, input, BN=False, err_BN=1e-8):\n",
    "        # only use it in predict data\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, delta):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta)\n",
    "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        # return delta_ for next layer\n",
    "        return delta_\n",
    "    \n",
    "    \n",
    "    # backward with batch normalization calculate grad_gamma and grad_beta\n",
    "    # gamma = ∆J/∆y * X ; beta = ∆J/∆y ; ∆J/∆y = y - y_hat because  f_derivative=1\n",
    "    # formula from: https://chrisyeh96.github.io/2017/08/28/deriving-batchnorm-backprop.html\n",
    "    # https://arxiv.org/pdf/1502.03167.pdf\n",
    "    def backward_BN(self, delta):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta)\n",
    "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        # return delta_ for next layer\n",
    "        return delta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, n_in, n_out, W=None, b=None, activation='relu', weight_norm=False, \n",
    "                 dropout=False, keep_prob=0.5, weight_decay=False, weight_lambda=0.01):\n",
    "        # initial stage\n",
    "        self.input = None\n",
    "        self.activation = Activation(activation).f\n",
    "        self.activation_deriv = Activation(activation).f_deriv\n",
    "        \n",
    "        # we use He initialization\n",
    "        self.W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n",
    "        self.b = np.zeros(n_out,)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        # set whether implement weight normalization\n",
    "        self.weight_norm = weight_norm\n",
    "        \n",
    "        # set whether implement weight decay and init\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_lambda = weight_lambda\n",
    "        \n",
    "        # set whether implement dropout\n",
    "        self.dropout = dropout\n",
    "        if dropout is True:\n",
    "            print(\"Using dropout\")\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # batch normalization init gamma and beta\n",
    "        self.gamma_BN = np.ones((1, n_in))\n",
    "        self.beta_BN = 0 #np.zeros(n_in,)\n",
    "        self.grad_gamma_BN = np.ones(self.gamma_BN.shape)\n",
    "        self.grad_beta_BN = 0 #np.zeros(self.beta_BN.shape)\n",
    "        self.BN_mean_total = []\n",
    "        self.BN_var_total = []\n",
    "        \n",
    "        # momentum init v and b params\n",
    "        self.v_w = np.zeros(self.W.shape)\n",
    "        self.v_b = np.zeros(self.b.shape)\n",
    "        # momentum with batch normalization init\n",
    "        self.v_gamma_BN = np.zeros(self.gamma_BN.shape)\n",
    "        self.v_beta_BN = 0\n",
    "\n",
    "    def forward(self, input, BN=False, err_BN=1e-8):\n",
    "        # judge the weight norm and implement\n",
    "        if self.weight_norm is True:\n",
    "            self.W = self.W / (((self.W ** 2).sum(axis = 0, keepdims = True)) ** 0.5)\n",
    "            self.W = np.nan_to_num(self.W)\n",
    "        # judge the dropout and implement\n",
    "        if self.dropout is True:\n",
    "            # set random probability matrix in every double layer\n",
    "            prob = np.random.rand(input.shape[0], input.shape[1])\n",
    "            prob = prob < self.keep_prob\n",
    "            input = input * prob\n",
    "        # judge whether weight decay and implement\n",
    "        if self.weight_decay is True:\n",
    "            self.W = self.W + self.weight_lambda * np.sum(self.W ** 2, axis=0) / 2\n",
    "        # judge whether normalization and implement\n",
    "        if BN is True:\n",
    "            input_mean = input.mean(axis=0, keepdims=True)\n",
    "            input_var = input.var(axis=0, keepdims=True)\n",
    "            input = (input - input_mean) / np.sqrt(input_var + err_BN)\n",
    "            input = input * self.gamma_BN + self.beta_BN\n",
    "            self.BN_mean_total.append(input_mean)\n",
    "            self.BN_var_total.append(input_var)\n",
    "        # calculate this layer output\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    def forward_predict(self, input, BN=False, err_BN=1e-8):\n",
    "        # judge whether batch normalization and implement it use average of all training batches mean and variances\n",
    "        if BN is True:\n",
    "            input_mean = np.mean(self.BN_mean_total)\n",
    "            input_var = np.mean(self.BN_var_total)\n",
    "            input = (input - input_mean) / np.sqrt(input_var + err_BN)\n",
    "            input = input * self.gamma_BN + self.beta_BN\n",
    "            # input = np.dot(input, self.gamma_BN) + self.beta_BN\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    # input is from previous layer, it is a_pre\n",
    "    # delta is dz, delta_ is next layer's delta\n",
    "    def backward(self, delta):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta)\n",
    "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        # return delta_ for next layer\n",
    "        return delta_\n",
    "\n",
    "    # backward with batch normalization calculate grad_gamma and grad_beta\n",
    "    # gamma = ∆J/∆y * X ; beta = ∆J/∆y ; ∆J/∆y = y - y_hat because  f_derivative=1\n",
    "    # formula from: https://chrisyeh96.github.io/2017/08/28/deriving-batchnorm-backprop.html\n",
    "    # https://arxiv.org/pdf/1502.03167.pdf\n",
    "    def backward_BN(self, delta):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta)\n",
    "    \n",
    "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        \n",
    "        self.grad_gamma_BN = np.mean(delta_, axis=0, keepdims=True) * self.gamma_BN\n",
    "        self.grad_beta_BN = np.mean(delta_)\n",
    "        \n",
    "        delta__ = delta_ * self.gamma_BN\n",
    "        # delta__ = np.nan_to_num(delta__)\n",
    "        # return delta_ for next layer\n",
    "        return delta__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, input_layers, hidden_layers, output_layers, activation='relu', weight_norm=False, \n",
    "                 dropout=False, keep_prob=0.5, output_softmax_crossEntropyLoss=False, weight_decay=False, weight_lambda=0.01):\n",
    "        # initialize layers\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "\n",
    "        self.activation = activation\n",
    "        self.weight_norm = weight_norm\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_lambda = weight_lambda\n",
    "\n",
    "        if self.weight_norm is True:\n",
    "            print(\"Using weight normalization\")\n",
    "\n",
    "        # init layers\n",
    "        # firstly add input layer\n",
    "        self.layers.append(InputOuput_Layer(\n",
    "            input_layers, hidden_layers[0], activation=activation, weight_norm=weight_norm\n",
    "        ))\n",
    "        # secondly add hidden layers if it more than two\n",
    "        if len(hidden_layers) >= 2:\n",
    "            for i in range(len(hidden_layers) - 1):\n",
    "                self.layers.append(HiddenLayer(\n",
    "                    hidden_layers[i], hidden_layers[i + 1], activation=activation, weight_norm=weight_norm, \\\n",
    "                    dropout=dropout, keep_prob=keep_prob, weight_decay=self.weight_decay, weight_lambda=self.weight_lambda\n",
    "                ))\n",
    "        # finally add output layer\n",
    "        self.layers.append(InputOuput_Layer(\n",
    "            hidden_layers[-1], output_layers, activation=activation, weight_norm=weight_norm\n",
    "        ))\n",
    "\n",
    "        self.output_softmax_crossEntropyLoss = output_softmax_crossEntropyLoss\n",
    "        if self.output_softmax_crossEntropyLoss is True:\n",
    "            print(\"Using softmax and cross entropy loss in output\")\n",
    "\n",
    "    def forward(self, input, BN, err_BN):\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(input, BN, err_BN)\n",
    "            input = output\n",
    "        return output\n",
    "\n",
    "    def forward_predict(self, input, BN=False, err_BN=1e-8):\n",
    "        # only use it in predict\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward_predict(input, BN, err_BN)\n",
    "            input = output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self, y, y_hat):\n",
    "        if self.output_softmax_crossEntropyLoss is False:\n",
    "            activation_deriv = Activation(self.activation).f_deriv\n",
    "            # MSE\n",
    "            error = y - y_hat\n",
    "            loss = error**2\n",
    "            # write down the delta in the last layer\n",
    "            delta = -error * activation_deriv(y_hat) / 512\n",
    "            loss = np.sum(loss)\n",
    "\n",
    "        # softmax and cross entropy loss\n",
    "        elif self.output_softmax_crossEntropyLoss is True:\n",
    "            activation_deriv = Activation(self.activation).f_deriv\n",
    "            exps = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n",
    "            # print(np.max(y_hat, axis=0))\n",
    "            last_y_out = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "            error = (last_y_out - y)\n",
    "            loss = -np.sum(y * np.log(last_y_out), axis=1, keepdims=True)\n",
    "            # write down the delta in the last layer\n",
    "            delta = error * activation_deriv(y_hat) / 512\n",
    "        # return loss and delta\n",
    "        return loss, delta\n",
    "\n",
    "    def backward(self, delta):\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)\n",
    "\n",
    "    def backward_BN(self, delta):\n",
    "        # when implement batch normalization use it not above backward function\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward_BN(delta)\n",
    "\n",
    "    def update(self, lr, momentum, gamma_MT):\n",
    "        # Update without momentum\n",
    "        if momentum is False:\n",
    "            for layer in self.layers:\n",
    "                layer.W -= lr * layer.grad_W\n",
    "                layer.b -= lr * layer.grad_b\n",
    "        # Update with momentum\n",
    "        elif momentum is True:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                layer.v_w = gamma_MT * layer.v_w + lr * layer.grad_W\n",
    "                layer.W -= layer.v_w\n",
    "                layer.v_b = gamma_MT * layer.v_b + lr * layer.grad_b\n",
    "                layer.b -= layer.v_b\n",
    "\n",
    "    def update_BN(self, lr, momentum, gamma_MT):\n",
    "        # when implement batch normalization use it not above update function\n",
    "        # Update without momentum\n",
    "        if momentum is False:\n",
    "            for layer in self.layers:\n",
    "                layer.W -= lr * layer.grad_W\n",
    "                layer.b -= lr * layer.grad_b\n",
    "                layer.gamma_BN -= lr * layer.grad_gamma_BN\n",
    "                layer.beta_BN -= lr * layer.grad_beta_BN\n",
    "\n",
    "        # Update with momentum\n",
    "        elif momentum is True:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                layer.v_w = gamma_MT * layer.v_w + lr * layer.grad_W\n",
    "                layer.W -= layer.v_w\n",
    "                layer.v_b = gamma_MT * layer.v_b + lr * layer.grad_b\n",
    "                layer.b -= layer.v_b\n",
    "                layer.v_gamma_BN = gamma_MT * layer.v_gamma_BN + lr * layer.grad_gamma_BN\n",
    "                layer.gamma_BN -= layer.v_gamma_BN\n",
    "                layer.v_beta_BN = gamma_MT * layer.v_beta_BN + lr * layer.grad_beta_BN\n",
    "                layer.beta_BN -= layer.v_beta_BN\n",
    "\n",
    "    # mini batch training\n",
    "    def mini_batches_random(self, X, y, mini_batch_size):\n",
    "        # the feature at columns\n",
    "        num_samples = X.shape[0]\n",
    "        mini_batches = []\n",
    "\n",
    "        permutation = list(np.random.permutation(num_samples))\n",
    "        rand_X = X[permutation, :]\n",
    "        rand_y = y[permutation, :]#.reshape(num_samples, 10)\n",
    "\n",
    "        num_complete = num_samples // mini_batch_size\n",
    "\n",
    "        for i in range(num_complete):\n",
    "            mini_batch_X = rand_X[i * mini_batch_size: (i + 1) * mini_batch_size, :]\n",
    "            mini_batch_y = rand_y[i * mini_batch_size: (i + 1) * mini_batch_size, :]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        if num_samples % mini_batch_size != 0:\n",
    "            mini_batch_X = rand_X[num_complete * mini_batch_size:, :]\n",
    "            mini_batch_y = rand_y[num_complete * mini_batch_size:, :]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        return mini_batches\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.1, epochs=100, gd='mini_batch', \n",
    "            momentum=False, gamma_MT=0.9, mini_batch_size=64, batch_norm=False, err_BN=1e-8):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        self.BN = batch_norm\n",
    "        if self.BN is True:\n",
    "            print(\"Using batch normalization\")\n",
    "        self.err_BN = err_BN\n",
    "        if momentum is True:\n",
    "            print('Using momentum')\n",
    "\n",
    "        # Implement Gradient Descent\n",
    "        if gd == 'GD':\n",
    "            print(\"Using GD\")\n",
    "            for k in range(epochs):\n",
    "                epoch_start_time = time()\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i = np.random.randint(X.shape[0])\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i], batch_norm, err_BN)\n",
    "                    # backward pass\n",
    "                    loss[it], delta = self.criterion_MSE(y[i], y_hat)\n",
    "                    self.backward(delta)\n",
    "                    # update\n",
    "                    self.update(learning_rate, momentum, gamma_MT)\n",
    "                to_return[k] = np.mean(loss)\n",
    "                epoch_end_time = time()\n",
    "                print(\"---------------------------------------\\n--- the epoch %s loss: %4f and used: %2f seconds\" \n",
    "                          % (str(k+1), to_return[k], (epoch_end_time - epoch_start_time)))\n",
    "\n",
    "        if gd == 'SGD':\n",
    "            print(\"Using SGD\")\n",
    "            for k in range(epochs):\n",
    "                epoch_start_time = time()\n",
    "                loss = 0\n",
    "                i = np.random.randint(X.shape[0])\n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i], batch_norm, err_BN)\n",
    "                # backward pass\n",
    "                loss, delta = self.criterion_MSE(y[i], y_hat)\n",
    "                self.backward(delta)\n",
    "                # update\n",
    "                self.update(learning_rate, momentum, gamma_MT)\n",
    "                to_return[k] = np.mean(loss)\n",
    "                epoch_end_time = time()\n",
    "                print(\"---------------------------------------\\n--- the epoch %s loss: %4f and used: %2f seconds\" \n",
    "                          % (str(k+1), to_return[k], (epoch_end_time - epoch_start_time)))\n",
    "\n",
    "        # Implement Mini Batch\n",
    "        if gd == 'mini_batch':\n",
    "            print(\"Using mini batch\")\n",
    "            print(\"the mini batch size is {}\".format(mini_batch_size))\n",
    "            for k in range(epochs):\n",
    "                epoch_start_time = time()\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                # seed = k\n",
    "                mini_batches = self.mini_batches_random(X, y, mini_batch_size)\n",
    "\n",
    "                for it, mini_batch in enumerate(mini_batches):\n",
    "                    loss_it = 0\n",
    "                    (mini_batch_X, mini_batch_y) = mini_batch\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(mini_batch_X, batch_norm, err_BN)\n",
    "                    # backward pass\n",
    "                    loss_it, delta = self.criterion_MSE(mini_batch_y, y_hat)\n",
    "                    loss[it] = np.mean(loss_it)\n",
    "\n",
    "                    # judge BN\n",
    "                    if batch_norm is False:\n",
    "                        self.backward(delta)\n",
    "                        # update\n",
    "                        self.update(learning_rate, momentum, gamma_MT)\n",
    "                    elif batch_norm is True:\n",
    "                        self.backward_BN(delta)\n",
    "                        # update\n",
    "                        self.update_BN(learning_rate, momentum, gamma_MT)\n",
    "\n",
    "                to_return[k] = np.mean(loss)\n",
    "                epoch_end_time = time()\n",
    "                print(\"---------------------------------------\\n--- the epoch %s loss: %4f and used: %2f seconds\" \n",
    "                          % (str(k+1), to_return[k], (epoch_end_time - epoch_start_time)))\n",
    "        # return to_return\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x, model):\n",
    "        x = np.array(x)\n",
    "        output = model.forward_predict(x, self.BN, self.err_BN)\n",
    "        # when implement output softmax, do it\n",
    "        if self.output_softmax_crossEntropyLoss is True:\n",
    "            exps_out = np.exp(output)\n",
    "            output = exps_out / np.sum(exps_out, axis=1, keepdims=True)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAcc(testLabel, realLabel):\n",
    "    acc_num = 0\n",
    "    for i in range(len(testLabel)):\n",
    "        if (testLabel[i] == realLabel[i]):\n",
    "            acc_num += 1\n",
    "    acc = acc_num / len(realLabel)\n",
    "    print(\"\\n--- the accuracy is: {:.4%}\".format(float(acc)))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def predict_data(test_data, model):\n",
    "    predictLabel = model.predict(test_data, model)\n",
    "    predict_array = np.array(predictLabel)\n",
    "    predictLabel = (predict_array == predict_array.max(axis=1, keepdims=1)).astype(float)\n",
    "    predictLabel = [np.where(r == 1)[0][0] for r in predictLabel]\n",
    "    return predictLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(inputDat, labelData, precentage=0.8):\n",
    "    data_training_shape = int(inputDat.shape[0] * precentage)\n",
    "\n",
    "    data_index = np.random.permutation(inputDat.shape[0])\n",
    "    data_training_index = data_index[:data_training_shape]\n",
    "    data_testing_index = data_index[data_training_shape:]\n",
    "\n",
    "    trainDat = inputDat[data_training_index]\n",
    "    trainLabel = labelData[data_training_index]\n",
    "\n",
    "    testDat = inputDat[data_testing_index]\n",
    "    testLabel = labelData[data_testing_index]\n",
    "    return trainDat, trainLabel, testDat, testLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "trainDat, trainLabel, testDat, testLabel = splitData(input_data, label_data, precentage=0.9)\n",
    "realLabel = [np.where(r == 1)[0][0] for r in testLabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weight normalization\n",
      "Using dropout\n",
      "Using dropout\n",
      "Using softmax and cross entropy loss in output\n",
      "Using momentum\n",
      "Using mini batch\n",
      "the mini batch size is 512\n",
      "---------------------------------------\n",
      "--- the epoch 1 loss: 0.004371 and used: 0.716923 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 2 loss: 0.003827 and used: 0.711948 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 3 loss: 0.002997 and used: 0.617819 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 4 loss: 0.001394 and used: 0.634693 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 5 loss: 0.001050 and used: 0.620425 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 6 loss: 0.000969 and used: 0.622312 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 7 loss: 0.000927 and used: 0.658476 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 8 loss: 0.000900 and used: 0.635242 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 9 loss: 0.000881 and used: 0.612682 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 10 loss: 0.000865 and used: 0.659552 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 11 loss: 0.000856 and used: 0.639504 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 12 loss: 0.000845 and used: 0.653906 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 13 loss: 0.000837 and used: 0.660984 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 14 loss: 0.000830 and used: 0.642634 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 15 loss: 0.000827 and used: 0.655762 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 16 loss: 0.000820 and used: 0.655342 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 17 loss: 0.000817 and used: 0.663308 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 18 loss: 0.000812 and used: 0.642606 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 19 loss: 0.000809 and used: 0.671469 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 20 loss: 0.000806 and used: 0.698268 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 21 loss: 0.000804 and used: 0.692553 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 22 loss: 0.000804 and used: 0.683742 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 23 loss: 0.000797 and used: 0.642526 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 24 loss: 0.000798 and used: 0.650852 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 25 loss: 0.000795 and used: 0.630457 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 26 loss: 0.000793 and used: 0.653931 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 27 loss: 0.000792 and used: 0.647021 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 28 loss: 0.000790 and used: 0.656123 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 29 loss: 0.000789 and used: 0.660090 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 30 loss: 0.000786 and used: 0.678066 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 31 loss: 0.000784 and used: 0.661806 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 32 loss: 0.000783 and used: 0.673235 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 33 loss: 0.000784 and used: 0.674567 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 34 loss: 0.000780 and used: 0.667096 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 35 loss: 0.000779 and used: 0.663703 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 36 loss: 0.000781 and used: 0.635311 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 37 loss: 0.000776 and used: 0.636681 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 38 loss: 0.000775 and used: 0.671645 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 39 loss: 0.000775 and used: 0.635297 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 40 loss: 0.000772 and used: 0.629564 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 41 loss: 0.000774 and used: 0.672633 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 42 loss: 0.000773 and used: 0.650571 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 43 loss: 0.000772 and used: 0.670912 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 44 loss: 0.000770 and used: 0.673987 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 45 loss: 0.000777 and used: 0.694707 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 46 loss: 0.000771 and used: 0.717406 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 47 loss: 0.000768 and used: 0.686324 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 48 loss: 0.000764 and used: 0.677117 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 49 loss: 0.000767 and used: 0.681481 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 50 loss: 0.000763 and used: 0.684560 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 51 loss: 0.000765 and used: 0.678492 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 52 loss: 0.000765 and used: 0.700270 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 53 loss: 0.000764 and used: 0.688037 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 54 loss: 0.000762 and used: 0.707631 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 55 loss: 0.000764 and used: 0.676883 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 56 loss: 0.000761 and used: 0.664856 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 57 loss: 0.000761 and used: 0.715109 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 58 loss: 0.000760 and used: 0.683307 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 59 loss: 0.000761 and used: 0.690062 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 60 loss: 0.000761 and used: 0.697410 seconds\n",
      "------------------\n",
      "\n",
      "last loss:0.000761\n",
      "--- Relu Fit used: 39.978075 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvcAAAENCAYAAACLnkpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3X98HVWd//HXJz/7I6U/AtRAK622qMWVdq2AiBpBobCu1V2Q4qqw8l10BX+vCruKiFsV3RVXF1xREBZhgUVdqltFBKKiQgFBpUChSwstLRT6k/RH0iSf7x/n3GQymZvcJPfmJrnv5+ORR+49c2bmzLnz4zNnzsyYuyMiIiIiImNfVbkLICIiIiIixaHgXkRERERknFBwLyIiIiIyTii4FxEREREZJxTci4iIiIiMEwruRURERETGibIE92Z2lpm5mZ1V4vksNrPbzOz5OL8HSzm/UjGz5lj+i8pdlmKKy9QyyHGujuPNKUmhisjMTjSz35rZ9ljm/yl3mWR0Gco2ICKFGWvbl5nNiWW+utxlGSlm1mJmeiY7xY31xm3LvZkdAPwvcBRwA/B54D/KWqg8KnGDzsfMLop10VzusgxHPPm4BZgLfI+w/t0Qh2lnViSj4WRvvJ58D0Ziu51T7rJIfuNl/yoi/aspdwFK6CjgYOCf3P2L5S7MMK0CXgE8X+6CFNkrgD3lLkSJvBmYAHzC3a8vd2FERGTUe5pwXNxZ7oLI2Daeg/tD4v9NZS1FEbj7HuDRcpej2Nx93C1TwrhZ/0REpPTcfT/j8FgvZeDu/f4BcwAHrgYOB24EtgBdQHMi3wzgS8AjwF7CmeftwIkZ0zwrTvOsVLoDLXnKcXUcPqfA8mb9nRXztIRFzxw/X9nWx79JwFeBp4A2YC3wacDyTO+oWGdPx/ybgZ8D74zDLyqgvM3x+0UZ058P/GecfjshmPxPYH5G3ty8moFTCVcE9gDbCF1GDh1ofYjTOSlOZ3kq/fhE2Wenht0U01+S7/eO9ZtZF1nrAfB+4E/APuBZ4ApgaiHLEKc1Bfgs8BCwC3gB+L/4e706I/87gV8R1u29cd4XAPWJPLnfKvP37GdYuh7WAw3ApcCGOL8HgbfHPDXAPwKPx+X/P+C8jDLXAecBK4EnCevgNuAXwMkZ+T8ey/ODjGFvBjrjck8ssI6bgMvi8rQDzwE/zFO/F8V5N2cMmxOHXZ1af7L+1ifytMS0euCfgXWxDv4P+BxQN9B8UsNbyF4fs/76LEfG9Prs8wgnhhcCvwGeoWe7vh54RSrvy+M07uhnHn8C9gMvytiOVxKuCObq5KvAtIxp5NbJA4Cvxc/7ifukxG83JzXe2wjHgc1xHpuAXwIfLHQ7jdM5PU5nG2F9Xw/8F7A4kecserazJfG32pn8vWK+E4CfJab1GPBlMvYdwEsI+5W1hG1wW6zP/wAaU9vZh4HfA9sJ+9X1hK55bx7Eck4i7FMeBHYDrcDvgDMy8jbH5b0IWEjohrojzvuXwLEZv2Gh+9eXAB8C/hiXO7l/qgI+ANwby7c7fv57oCrfOk5Yr68lxA97gfuBd6XyLon5r8pTP/WE9fV5EvvdwWxfMX0qIV5ZE9eB7cCtWb8VYMCZwG8J+699hH3yrcDpqbyviuvlesL6/lxcJ74O1BZQ3jlk7H8o4nEvTq8G+CBwN+HYtwd4gHCsyPoNzwJ+ADwRf7tdhP3Tu/uZxwxgOeH4uoewLf6BsK1NTuRricuWPKa1xTq+hNQ+uoTb0GsJx8WdhFjgVhL7l6GuP4lxTgR+TFj/c8vXa//AILfp/v4G03L/UuAewo7wOmBi/IExs8PiDzQH+DVhxzkZeCvwMzN7v7t/ZxDzGo4dhP7NC4GlhMrL3Ug73BtqawmB+SHAT4EO4O2ElXVCnG83M/s74FuEgGgFYaU9GFhM2LBuItTbNOAjhBU/edNlv+U1s9cQVsYpcfoPEw72fwMsNbMT3P2+jFE/SDjoriCsMEcTDp5HmtlCd28boB5+TQg4TgD+KZF+fOLzCYQdEmZmhJV2vbs/0c90v06ozzcC1xB2kPl8hRCc/Jjwm7wJ+DtgXqocmWKZfgYcS9jwv0v4PWfHsv6acPDJ5f8iYYfxPCHIagVOBr4InGRmb/HQ6rKesB40ZyzHg3HYWcBh9F5f0staC9xG2EHeQggezgB+YGYnEn7DownrYRtwGvBNM3vO3W9MTGcG8G+EA9NthINNE/CXwEoz+zt3/24us7t/LfbH/Ssz+6C7Xx6X/0XA9wk7stPdfW++uk3U2VzgLsL2cgfhoDc7lvUvzOyv3f0nA02nH58nrC9HxmXcEdN3ZOS9CXgNcDMhKF1K2IEuNrO3edyzDkFuez2TsC21JIatH+I03wCcD9xJOKC2Ek7iTwXeZmavc/c/QLj6ZWZ3Am8ys8Pd/bHkhMzsWOCVhJO1ZxLpFxLqbxvwE8IB51XAPwCnmNlr3X1Xqlx1hN9xBmGb20U4WcpkZucA3yacoPyYsO0cHOfzt8DlA1VE3E6/R6jf5wknhs8Bswjb/BogvY87lRAk/pQQhM9JTO/9hH3ybuC/43I3Expo/jLW7Y6Yt4kQtB5AOAn6AWE/Pxd4D/DvwNY46asJ2+dDhMaVvYT1/rhYll8UsKzTCPW7iBAQXkUIpE8CrjezI9z9MxmjLgY+Rc9+7MXAXwO3x/35mphvMPvXfwNeTwguVhKOYTnXAu8iBCffJQQj7yD8nscRjj9p0wn7oB2E33MaobHkOjM71N2/GvPdSjjJPN3MPubu6e4pfw00Av9awHEqU6zn3wALCL/v14EDY3l+bmZ/7+7fToyynLDvX0fYj+wk7ENfQ9iX3Rin+ypCjOSEY+s6wrozj7C//gxh3zMcwzruxXLWxvFPImw/1xP2628Cvkk4rrwnNdq3CPHFrwgn6o3AKcC1ZvYyd/9sah5zCfuvwwjH0W8R1uXDgY8RtsvdqXlcT1jnfkrYt5xCWK8PJuwvClm2oW5DRxN+418QGqPmAX8FvMHMTnT3X6fmMZj1BzP7PKHBppVwzNhA2D8cC7ybvvuHQrfp/AZxJunAF/PkaSG05C9LpU8jBDR7gZmps0CnBC33A80jeaY4mPHoafVYSaLVkrDi7Yh/tYn0BYQNeRtwRMZ8ZmXU8dV5ytRMquWe0JrwSEz/m1T+02P6oyTOwulpXdsF/FlqnOvjsHcWWL+/IgTDUxNpvyNsUM8D1ybSj4zTvnKg35t+Wm9T68FTwIsT6TWxTA4cVUD5/yzm/VHGsCpgeuL7axPzfFFqnj+Ow/6x0OXob/1LrWs/pvdVgdfH9G2Encq0xLCXEE64HkhNqz65riXSpxICkW2kWuEJO+7c1YIjY338Is77bwtZP+J0bo3j/FMq/di47mwFGgqsszkM0KKVpwwtcfhjqd90QlxfHXjPILbFPr8d/VxZK6COsraBg4EpGXmPJBwcfppKPzVO51/62V7ekkh7U0z7LalWenr2f5fmWSd/QaLVbYBlu59w4nlwxrADC5zGOXG+q0i1TgLVQFNG2buAJRnTOiyWZxfw8tSwy+O4VyTSPhTTPpIxrcnE7YawLXURTjKqM/I2Frisud/qU6n0CYSGiC5gYcZ6l3W8en9MvzyVfhGF7V+fBuZmDD8jDv89vbfdyXH5nb6t8bky3kTv49Fcwv6nnd5XdP8h5s+6EtkShx0+jO3r2zH92ySuuBNOoHfGdWROIn0rsBGY1N96DPxrnO7SjHzTyWgRz8g3h/73c8M67qXWgW8m11fC9nRl1jIAL82YTh3hatp+Ulf9CcGvAxdk1RkwIeM3vR+YkVqn1hJOLF9U4LLl6mko29B5qXGWxvTHU+vtYNefE2P+J9L1FIfPylOegrbpvHUxiJXtGTIug9ETuP13nvFzFfTBRNpZeQrfZ0PM+NHmDFTm/uaRXJkGMx49B7d5GeNcE4e9MpH2zZj2sUHU8dV5hud+8IsSaa+Lab/NM86v4/A3JNIuimn/nJE/d8DvEyDkmf7nYv63xe9TCBv5JYQWsU2JvLmuHlk7/ZZUWq6MzQOsB/8vY9jfkuegkJE3F9xfX0De78S852QMO5yw83mi0OXob/1LrWtZO9Qn4rDjM4bdSQia+wQYeeaT+13ekDHsuDitRwlXJxz4fiHTjePPiuM8ScblaELrnwPvLbDOMrcRCg/u35MxLLdd3TnQfPr77ShycD9A/hWEVrZkQ0INIRjr1VWB0Liyh3CATB6EfhTn26fRIQ5/ANiSZ508chBlvZ/QOje90HEypvGnON9FBeQ9izwn7HH4P5GnkYoQfO0inNDWx7RccN9nu0+Ne0DM9xvydM8soOyNcXu7N8/w3HH2Kxnr3V0Z+WsJ++P7Uul5t7E4PLc99TmhicNvi8OzutueEIfdkUr3uGxZJwu58nwuVRd7gT+l8r4sa/oD1Guv7SvWy25Ct4sZGfm/EMe5MJG2ldAK3283IHqC+z51M4jyzqH//dxwj3tVhP3EZqAmY/g0QgB8U4Hl/Sv67sdfHdMeoLATmpaYP6tL1OfjsLcWMJ3hbEOPZ5U1UbY3DmP9yTUAvqOAZciVp+BtOt/fYLrl/MGzL4O9Nv6fmudRcAfF/68YxLxGq53uvjYjfUP8Pz2Rdkz8/9MSleXP4/878gy/gxCgLSKc2SdlddXJWob+3EHYMZ9ACDjeSAgybicEAqea2Svc/RF6LhfmK+tQDHcZHiZcVTojdiu7hdCF5D53b0/lzVvX7v6YmW0E5prZNI+X9Itgh7v/X0b6JkKL1/0Zw54mtL68KH4GwMyOAD5J6O7RRGjFSDo0PSF3v8vMPkfop34BYef3gUGUf1H8/2sP3ZXS7iBcjlxE6MZQar/MSPs14WCwKGNYWZnZXxDqezGhpSu9rz6QcIDG3TvM7LuEy75/TbgKB+HS+kRCa7Qnxn0t4SBxmpmdljH7OuAgM2t0962J9H2EPtiFuo4Q8Kw2sxsJv8Fv3P25QkY2s8mELkXPuvsDg5jvqjzp/W3H283sAcI28nJCF8kVhBPby8zsJMKVqN8ADyfr0913mdmPCV3dHjSzHxDWrXs8PAyhEK8hbLv5HqlaG/9nHUf77Avdfb+ZPUvh+/O0/uqwi97dz3J+SWjoyNqennL3dRnpLYSGou5x3H2rmd0EvNfMjnX338ZB58T/w3mk9csJfbJ/4+7bMobfQeg+k1yG6wgneqvN7L8Jy/k779tl6EZC99r/MbObCVe5fpNnPz5Uwz3uHU4Igh8HPhN6vfWxl9R6ZmYvJnRdO4HQRWRiapzkMSQX+9zq7l0FlClnuMs2nG3o13nK2kKIbRYRfvehrD/HEAL2nw28CN2GvU0PJrh/Jk96Y/z/lviXT8Mg5jVa5QvcOuL/6kTatPj/aUpjavy/Oc/wXPq0jGFZy5G1DP25m3AGe0L8fgLh8upd9PTlPMHMHiccMB/2RJ/fIhjWMrh7p5kdTwiITiVccQB4wcyuIVxObI1phdT1i2O+YgX3+R6F1gGQcWDpHkbPTgwzO4aww8mdeK0gtFB20XNfSn2eef0QuJjQ2vPdRH0UYjjrZyk8m06I68BWQjeYUcPMPkzo87yd0FL6FKEF3um5xyD9m11BuBnt/fQE9+cQtsnvpfI2EtaHzw1QlAZ6+pRDaM33fJnTPNy/8Tyhv/GHgY8SDry/BD7p2fcDJQ11H5pvPzOoddLdnzSzowiNGEsIrZQAG8zsX9z9G4lxTycEP++i516afTHI+wd377P+peSOo6+Jf/lkHUf7Oy4Vuj9P668Ot2U0gOROMnP3VaTlW/7cfKam0i8H3ktYn39rZvWE+y620Pu+tMEayn7pY4T7AN5HuBfmfKDDzFYSHnW8FsDdV5nZ6wlXiE4l9ls3szXA5939v4ZR7pzhHrtz69l8+t/+u9czM3sJ4WRvOuGk9eeE41Mn4UrDmfTeHw1pu83TMDaUZRvKNlTo+jmU9WcasN0LuE8tYdjb9GCC+3w79VyQ8ZHUzm4onPxlKmYQ0AVgZjXu3pEaVqz55H6cQynNo61y9f6iPMObUvmKKp5F3kW4mbSJENz/LrZU5Vqz30zomzmF4rbaF4W7byfsuD9mZvMIZ+jvJzwxYBo9NxUl6zqrFaakdT1MnyG0srzJ3VuSA8zsAkJw34eZTSDcAAshyLzQzG7xQm7kCYayfuZaTrL2AcPdLmcSguRuZlZNOCAkbxztrwzFKEe/zKyGEBw+A/y5u29ODX9t1nju/nRsPX6Hmb2CcCB+JXBjRkv5TsIl6BmDLF7BgX2iXP8J/Ge8Ce1Ywo2X7wNujVf2tvQzenIfWoxyJtfJ1RnD+6yT8crj6fF3OZKwT/sQ8G9mttvdr4z59hJOAi4ys9mEBo2zCFen5hDul+lPbp6XuvvHB8g7EvqrwxlmVpu+Ihfr6EB6b085M/NML7d/6LXvdPd7zOz3wDvN7KOEhxc0ApdknVgMwqD3S+7eSTjZ/jczO5hwRXwZ4WbaI+JNmm0x7++At8aTkVcTTgo/RLiZ8zl3H/DG6hLLLdeP3P2v+s3Z4+OEuv9bd786OcDMziAE90lD3W6HazjbUKHr51COazuARjObOMgAf1iK8Ybau+P/gXZehdhOeJpGL/EgvLAI00/Oh6x5ES6DF0OuXk4uIG/uSQSDaWXJXaZuzjM8l/77QUxzsG6P/5cRAonbE8PuiGV4SyrvQIZSF8Pm7mvjgfqNhJsWk0Fv3rqOJwWzgHWD6JLTGccdiWWcR2hpa8kY9sZ+xvsaIZj5EuH3nQTcGIP+QuTq7Lh40E97U/yfXD+Hsl0Wur5kLevrCUF8sstH3jJYeOv14cMoQyEOJJxA/DYjsG+gp2tJltzTZ86hpwvDtzPy3Q1Mj921RoS773D3le7+d4T+wzMY4Jjh7rsJN33PNLNidJ3qbzueRjjG7CM8qCBdlg53v9/dLyHcVArhKkpWuTe4+3WEJ3Q8TtgGGrPyJqwinFgW4zjan+Guqw8Q4oY3ZAx7Q5xu1jHnxZb99uLmxHTTvkXoQvheem6sHu5T99YQroItNLOs7g1Z+6Vu7r7F3X/o7u8kHONeSjj2pfO1uftv3f1CwlUryNOQMsIeJQSbx8Sn5hRiXvz/g4xhWfvVXOxzkpkVI8Ys1HC2oePylLU5/s+tn0NZf+4mPABlyRDKNWTDrvh4afXXhEfnvS8rj5n9WTzjHcgqwk7gxFT6ZwhPOiiWXH/Cv0smmtkJ9Oy4h+tbhEsonzWzBemBZjYr8XU7Ycf14kFM/zeEFe04Mzs1Ne1TCTvaxwjdZEol1xp/PmHlTQf3UwmX5PP10cyS6wYwmLoYNDObmye4mU64xJg8w74q/v+MmeXuIckF5/9C2I6uHMTsR2QZo/WElrZXJRPN7GxC8NGHmf014ZnVvyHc6PZzwiPYjiQE/QNy942ELiVzCN0xktM/mtB9YTvh5s6c3Hb5t8kTgtgSemGeWRVal59N7ozjScqX4tfubivu/gLhAPi65HYbf+uv0bev6WDKUIgthIPHq2Mwn5t/LaH18MB+xr2dsM2fSXgs22PufmdGvkvj/++Y2SHpgWY2OXbnGhYzW5LnxC53LCikP3ruavC3zaxX1w0zq4pXDQv1fcK9Bh+KJ+VJXyDcGPv9XCusmR1lZlkterm0PTHfQXGdTptMuGrZQegelVe8gnEd4dGsn82qNzN7qYVHDA7HcNfV3L7wS2Y2KZcYP385fs3aF1YDlyQDqLgsHybUz/czxrme0AL6KUIQedtw+6/HVv/rCF0zLk4OM7OXxvLsJ9zwj5nVm9kJluqcHrfH3JWv3Hrw+vQ6GvVaX8op9lT4JqGF+Rtm1md/ZmZNqZhlffzfnMp3EvD/MuZxP+FJXAsJXdXS028cRCNRwYa5Dc0nxCrJvEsJ691aQow76PUn+mb8/69m1udqRlZaMRTrDbXvIgRzV8b+ovcQzg5nEZ5p/ErCTVz9XYKFECidBNxi4QasbYRLuXMJwWFzkcr7PcINhheY2ZGEmysPJ7Sy/4hwU9qwuPvDZvZBws0/D5jZLYRWnEZCK+QLxLM8d281s3uA15vZdYQDdCewwt0zb2BzdzezMwkB1I1x+o8Snijw9jj99w7yhpbBeoDwGx1MaO1O3oSVC/QPJtykWmir9p2Ek4EvmdkriS2p7v7PRSlxjyOBH5nZ/YTWwU2Em7+XEvqs5/rg4+6/NbOvEA4yD1noR7ubsL68knAC9VUKdzvhku4PLfTb3As86e7X9j/akHydsE3dZeEmtZ2E9e84wjPf0yeGcwjP1d1OeLpRrqXvM4QTxr83s9vdPasVJ+0DhBOEr8YT9vvoec59F+Ey7wu5zPFS/K/ifFaZ2R2EA+NfEm5mzGrRv52wLX8n/i6thJuR/z2V7xHCDXHJ59y/lPAc73S9f5UQoPzGwg10uWdA1xJutDwylX8NoX/pMjNrJ3T/ccLjYJ8csJYS3L3LzL5BOGH+U9yu6+L8ZxC2jzflGdfN7D/oOQHLarXH3W83s/MJJzePx3VwHeGAdRjhgHYXw29puoHQ7zx3H44RWtVeQ7ghvJAuCt8lrKvvjWW9hfCc+0MIN+pfRegOMyB3Xx+7eFwG/D5uD88Rlve1hP1nMhh5F3CuhXsE1hK2iZcS1sc2wrYFofvB3Wb2CKHFbgPhROGthMv330iu5/04jxBkXAy8J9bbs3FZX0GotzPo590CBRjW/tXdr49BzzsJ29P/0HMvyFzCU1auyxj1j4Rnid9vZj8nNPycTrhK9amsoN3d91i4/ynX8p25Pg/B+YT18DwL74q5k57nlE8hPHUmV8cTCevp+niMfpJwNeEthN9kRey6BfAJ4EQzayE81awVOIJwnNhOuC9mNPgCYR/2AcK7He4g7L8OJqx/ryPcN/BwzH854Yk8/23hZvGnCce9JYTHm56eMY93E2K2L8bGohbC9j+f8GjIlzP094D0Z6jb0M8IwffJhH187jn3+4CzU3HUYNYf3P3nZvYFwgszH4nbzAbCse04Qsv+WcWqgG4+xEczZeSbQrih637CSr2XUIH/S7iklnwj2VlxmmdlTOdthCBgH6GV4QbCAedq+nnkXcZ08s4jDj+C8Mz6F2J5Wwg7+czxiG9ozDOti8j/CL/XEi5nbaHnTZM/A05N5ZtHeGTSVsLOt7sM9P+G2pcRgpPNhKBlM6EV5GWDLGdBv3PGeD+I4/1vxrA1cdglecbt9ZiyRPq76Xk/gpN49GB/60F/9ZSRdxbhSRi5t4C2EZ5l/FMy3twax1lGCHpeiOvnasJOcEJG3v7qujrO+4n4m/WqhwHWtZZkfaSGZdYNIci4O5Z7B+GGqFyf4OR6VhvzOfBXGdM/jHCQ2kHGY+3ylOlQwlWsJ+P6/zzhhrjX5Mk/jXDpPfcWv4cI+4+86yehT+gjMb8n646eR5ml31D7BOGGsszH2wFnx9+3La4f3yacmGfWP+GgcTvh5Cm3/fb57QvZBgiNLh8nHFz3xvlfSwH7QcKVp864fvb7fHXCgeUmwj4p9/bgBwknB4tTefOuk/1M/wOExpIn6HkT9gOEk+Q+z/EfYFp/Q3haxc64bOsILWh/nsjTa33uZ1onxm1gOz1vGf8KfZ/5f3Rcd/8Qy7435v0evR97PI1wZSkXJOXeRN5CCCQKfjwmPW+U/i09z8x+Kq5bH6X3W3Gb6Wd/l+83Y4j710SeKkIr533xd91DOO6fy8BvqP0+YdveRzgRetcA9ZF7fOEmMh7dOJTtK/GbXULP21B3EBrKTkzlq43r60/j77CPsJ3cHdfvukTeE+O68XD87XYTjoHfAA4rsLxz6P9RmH1+l4HWgzzzMcI9Zbm3PrfHdfcuQhyXfsP8sXH93k44jtxFOKHLO2/iPRL0vMV1R1zvlpN4ZwBDeDR5KbYhet5Qm3tb/c/p/zg14PqTGucUet6MnXtD7Y9IPNZ6oN+SQeyHLY4gIjLuxFa0N7p75jPfxhsLbxe+k9C9JP2WSZERZ2YO/NLdm4cw7lmEgPmfPfUWVJHhSuwvP+/uF5W3NMU1kjc7iIhIaX0q/k93SxIZU2Kf6Y8T+uQXq0uOSEUoVp97EREpAzP7M0LXq1cT+vf+xN3vKW+pRIbGzI4jdJFtJrxJ/N893KAvIgVScC8iMra9mnAPxy7gv0k99UFkjHkz4X6YbYT7bz7Vf3YRSVOfexERERGRcaIiW+7NbAnhmdHVwHfd/cup4fXAfxJaxLYCp7v7+jjsAsKTNDqBD7v7rYnxqglPEHja3d8a064mXGLMvbHsLHd/sL/yHXjggT5nzpzhLeQAdu/ezeTJk0s6j0qkei0N1WtpqF5LQ/VaGqrX0ihGvd5///3Pu/tBA+eUkVBxwX0MwC8jPKd2I3Cvma1w94cT2c4Gtrv7PDNbRnjk0enxxQ7LCI/RPAT4hZkd7j3PAv8I4ZF8B6Rm+0l3v7nQMs6ZM4f77rtvKItXsJaWFpqbm0s6j0qkei0N1WtpqF5LQ/VaGqrX0ihGvZrZoN7pIaVViU/LOQpY6+5PeHjb2A30fS30UuCa+PlmIPeGuqXADR5eLb2O8Mzjo6D7jbN/QXjpioiIiIjIiKu4lnvCS3U2JL5vJLysJDOPu3eY2U7CCxkOJby8Ijlu7tXBXye+nCVjnsvN7ELCSxTO9/h68yQzO4fwsh5mzpxJS0vL4JZqkFpbW0s+j0qkei0N1WtpqF5LQ/VaGqrX0lC9jj+VGNxnvcwmfVdxvjyZ6Wb2VmCLu98fX4qQdAHhDZN1hNdPf5rwauTeE3G/Ig5n8eLFXupLj7q8WRqq19JQvZaG6rU0VK+loXotDdXr+FOJ3XI2ArMT32cRXm2dmSe+SGMq4bFc+cZ9HfA2M1tP6OZzvJl9H8DdN3vQRnjT3lHFXiAREREREajM4P5eYL6ZzTWzOsINsitSeVYAZ8bPpwJ3eHhm6ApgmZnVm9lcYD6wyt0vcPdZ7j4nTu8Od383gJk1xf8GvB14qLSLJyIiIiKVquK65cQ+9OcBtxIehXmVu682s4uB+9x9BXAlcK2ZrSW02C+L4642s5uAhwmvxD438aScfK4zs4MIXXoeBD5QkgUTERERkYpXccE9gLtYZMNSAAAgAElEQVSvBFam0i5MfN4HnJZn3OXA8n6m3QK0JL4fP7zSioiIiIgUphK75VQ073I2fmMj3FXukoiIiIhIsSm4rzBWZWz69ib4n3KXRERERESKTcF9BWo8pRH+AB2tHeUuioiIiIgUkYL7CjTjlBnQATtu31HuooiIiIhIESm4r0BTXzcVJsHWlVvLXRQRERERKSIF9xWoqq4KXg3bfrqN8Ph+ERERERkPFNxXqqOhbUMbu1fvLndJRERERKRIFNxXqqPCv20rt5W3HCIiIiJSNAruK9VBMPnIyep3LyIiIjKOKLivYI2nNLLzrp107NQjMUVERETGAwX3FWzGKTOgE7bdpq45IiIiIuOBgvsKdsAxB1AzrUb97kVERETGCQX3FayqporpJ07XIzFFRERExgkF9xWu8ZRG2p9pp/XB1nIXRURERESGScF9hZuxZAagR2KKiIiIjAcK7itc3cw6piyeokdiioiIiIwDCu6FGafMYNfdu9i/dX+5iyIiIiIiw6DgXmg8pRG6YNvP1TVHREREZCyryODezJaY2RozW2tm52cMrzezG+Pwe8xsTmLYBTF9jZmdlBqv2sweMLOfJNLmxmk8HqdZV8plG4opi6dQe2Ct+t2LiIiIjHEVF9ybWTVwGXAysAA4w8wWpLKdDWx393nApcAlcdwFwDLgCGAJcHmcXs5HgEdS07oEuNTd5wPb47RHFas2pp80nW0/24Z36ZGYIiIiImNVxQX3wFHAWnd/wt3bgRuApak8S4Fr4uebgRPMzGL6De7e5u7rgLVxepjZLOAvgO/mJhLHOT5OgzjNt5dkqYap8ZRG9j+/nxfue6HcRRERERGRIarE4P5QYEPi+8aYlpnH3TuAnUDjAON+HfgU0JUY3gjsiNPIN69RYcZJM8DQU3NERERExrCachegDCwjLd0XJV+ezHQzeyuwxd3vN7PmQc4rZDQ7BzgHYObMmbS0tGRlK5rW1ta+81gAT974JE82P1nSeY9nmfUqw6Z6LQ3Va2moXktD9VoaqtfxpxKD+43A7MT3WcCmPHk2mlkNMBXY1s+4bwPeZmanABOAA8zs+8B7gGlmVhNb77PmBYC7XwFcAbB48WJvbm4ezjIOqKWlhfQ81i9bz/rPrufYVxxL3cxRd9/vmJBVrzJ8qtfSUL2Whuq1NFSvpaF6HX8qsVvOvcD8+BSbOsINsitSeVYAZ8bPpwJ3uLvH9GXxaTpzgfnAKne/wN1nufucOL073P3dcZw74zSI07yllAs3HI2nNAKw7VY9NUdERERkLKq44D62oJ8H3Ep4ss1N7r7azC42s7fFbFcCjWa2Fvg4cH4cdzVwE/Aw8DPgXHfvHGCWnwY+HqfVGKc9KjUsbKB2Zi3bfqrgXkRERGQsqsRuObj7SmBlKu3CxOd9wGl5xl0OLO9n2i1AS+L7E8Qn6ox2VmU0ntzI87c8T1dHF1U1FXfuJyIiIjKmKXqTXmacMoOO7R28cI8eiSkiIiIy1ii4l16mv2U6VOuRmCIiIiJjkYJ76aV2Wi1TXzeVbSvV715ERERkrFFwL300ntJI64OttG1qK3dRRERERGQQFNxLH9OOnwbArt/tKnNJRERERGQwFNxLH5NfORmqofXB1nIXRUREREQGQcG99FE9sZpJL5+k4F5ERERkjFFwL5kaFjbwwgN6HKaIiIjIWKLgXjJNWTSF9qfbaX+uvdxFEREREZECKbiXTA0LGwD1uxcREREZSxTcSyYF9yIiIiJjj4J7yVTbWEv97HpaH1BwLyIiIjJWKLiXvBoWNajlXkRERGQMUXAveTUsbGDPmj107uksd1FEREREpAAK7iWvhoUN0AW7/7S73EURERERkQIouJe8GhbpploRERGRsUTBveQ14bAJ1Eyr0cusRERERMYIBfeSl5nRsFA31YqIiIiMFQrupV8NCxvY/cfdeKeXuygiIiIiMoCKDO7NbImZrTGztWZ2fsbwejO7MQ6/x8zmJIZdENPXmNlJMW2Cma0ysz+Y2Woz+3wi/9Vmts7MHox/C0diGYulYVEDXXu72PPYnnIXRUREREQGUHHBvZlVA5cBJwMLgDPMbEEq29nAdnefB1wKXBLHXQAsA44AlgCXx+m1Ace7+5HAQmCJmR2TmN4n3X1h/HuwhItXdHpTrYiIiMjYUXHBPXAUsNbdn3D3duAGYGkqz1Lgmvj5ZuAEM7OYfoO7t7n7OmAtcJQHuei3Nv6Ni34sk14xCaszvalWREREZAyoKXcByuBQYEPi+0bg6Hx53L3DzHYCjTH97tS4h0L3FYH7gXnAZe5+TyLfcjO7ELgdON/d29KFMrNzgHMAZs6cSUtLy1CXryCtra2Fz+Mw2HDnBja0bBg4b4UbVL1KwVSvpaF6LQ3Va2moXktD9Tr+VGJwbxlp6Vb2fHnyjuvuncBCM5sG/MjMXunuDwEXAM8AdcAVwKeBi/tMxP2KOJzFixd7c3NzQQszVC0tLRQ6j0df/yhbV2zl2DceS7iAIfkMpl6lcKrX0lC9lobqtTRUr6Wheh1/KrFbzkZgduL7LGBTvjxmVgNMBbYVMq677wBaCH3ycffNsdtOG/A9QregMaVhUQP7n99P+6b2chdFRERERPpRicH9vcB8M5trZnWEG2RXpPKsAM6Mn08F7nB3j+nL4tN05gLzgVVmdlBsscfMJgJvBh6N35vifwPeDjxU0qUrAd1UKyIiIjI2VFy3nNiH/jzgVqAauMrdV5vZxcB97r4CuBK41szWElrsl8VxV5vZTcDDQAdwrrt3xgD+mtjvvgq4yd1/Emd5nZkdROjS8yDwgZFb2uJoODIE9y888AKNf9FY5tKIiIiISD4VF9wDuPtKYGUq7cLE533AaXnGXQ4sT6X9EViUJ//xwy1vudVMqWHivIlquRcREREZ5SqxW44MQcPCBj0OU0RERGSUU3AvBWlY1MC+J/bRsbOj3EURERERkTwU3EtBum+q/YNa70VERERGKwX3UpCGRXpijoiIiMhop+BeClL3ojpqD65VcC8iIiIyiim4l4KYGQ2LdFOtiIiIyGim4F4K1rCwgd2rd9PV3lXuooiIiIhIBgX3UrCGhQ34fmf3w7vLXRQRERERyaDgXgo2ZdEUQDfVioiIiIxWCu6lYBPnTaRqUpWCexEREZFRSsG9FMyqjYYjdVOtiIiIyGil4F4GpWFhA60PtuLu5S6KiIiIiKQouJdBaVjUQOeuTvat21fuooiIiIhIioJ7GZSGhXpTrYiIiMhopeBeBmXyKydDNep3LyIiIjIKKbiXQameWM2kl09Sy72IiIjIKKTgXgYtd1OtiIiIiIwuCu5l0KYsmkLbxjban28vd1FEREREJKEig3szW2Jma8xsrZmdnzG83sxujMPvMbM5iWEXxPQ1ZnZSTJtgZqvM7A9mttrMPp/IPzdO4/E4zbqRWMZS0k21IiIiIqNTxQX3ZlYNXAacDCwAzjCzBalsZwPb3X0ecClwSRx3AbAMOAJYAlwep9cGHO/uRwILgSVmdkyc1iXApe4+H9gepz2mdQf3uqlWREREZFSpuOAeOApY6+5PuHs7cAOwNJVnKXBN/HwzcIKZWUy/wd3b3H0dsBY4yoNcpFsb/zyOc3ycBnGaby/Vgo2U2sZa6mfXq+VeREREZJSpKXcByuBQYEPi+0bg6Hx53L3DzHYCjTH97tS4h0L3FYH7gXnAZe5+j5kdCOxw9450/jQzOwc4B2DmzJm0tLQMdfkK0traOrx5HApb7t7ClpYtRSvTeDDsepVMqtfSUL2Whuq1NFSvpaF6HX8qMbi3jDQvME/ecd29E1hoZtOAH5nZK4FnC5gXcfwrgCsAFi9e7M3NzZmFL5aWlhaGM4/HFj3Glpu2cFzzccUr1Dgw3HqVbKrX0lC9lobqtTRUr6Wheh1/KrFbzkZgduL7LGBTvjxmVgNMBbYVMq677wBaCH3ynwemxWnkm9eYVNdUR8fWDrrau8pdFBERERGJKjG4vxeYH59iU0e4QXZFKs8K4Mz4+VTgDnf3mL4sPk1nLjAfWGVmB8UWe8xsIvBm4NE4zp1xGsRp3lLCZRsxdU3hoT/tz+hxmCIiIiKjRcUF97H/+3nArcAjwE3uvtrMLjazt8VsVwKNZrYW+Dhwfhx3NXAT8DDwM+Dc2B2nCbjTzP5IOHm4zd1/Eqf1aeDjcVqNcdpjXn1TPQDtmxXci4iIiIwWldjnHndfCaxMpV2Y+LwPOC3PuMuB5am0PwKL8uR/gvCEnnEl13LftrmtzCURERERkZyKa7mX4ujulqOWexEREZFRQ8G9DEndwXVQBe2bFNyLiIiIjBYK7mVIrNqoO7hO3XJERERERhEF9zJkdU116pYjIiIiMooouJchU3AvIiIiMroouJchU3AvIiIiMroouJchqz+knvYt7Xinl7soIiIiIoKCexmGuqY66IL2LWq9FxERERkNFNzLkOlZ9yIiIiKji4J7GbL6pnpAwb2IiIjIaKHgXoYs13KvZ92LiIiIjA4K7mXI6l6kbjkiIiIio4mCexmyqroqahpraN+k4F5ERERkNFBwL8NS31SvbjkiIiIio4SCexkWvchKREREZPRQcC/DouBeREREZPRQcC/DUtdUR/sz7bjrLbUiIiIi5abgXoal/pB6fL+zf+v+chdFREREpOIpuJdh0VtqRUREREaPigvuzWyJma0xs7Vmdn7G8HozuzEOv8fM5iSGXRDT15jZSTFttpndaWaPmNlqM/tIIv9FZva0mT0Y/04ZiWUcSQruRUREREaPmnIXYCSZWTVwGfAWYCNwr5mtcPeHE9nOBra7+zwzWwZcApxuZguAZcARwCHAL8zscKAD+IS7/97MpgD3m9ltiWle6u7/MjJLOPLqm+oBBfciIiIio0GltdwfBax19yfcvR24AViayrMUuCZ+vhk4wcwspt/g7m3uvg5YCxzl7pvd/fcA7v4C8Ahw6Agsy6iQa7nXs+5FREREyq+iWu4JQfeGxPeNwNH58rh7h5ntBBpj+t2pcXsF8bELzyLgnkTyeWb2XuA+Qgv/9qyCmdk5wDkAM2fOpKWlZRCLNXitra3Fm8dkWLdqHeta1hVnemNYUetVuqleS0P1Whqq19JQvZaG6nX8qbTg3jLS0s9wzJen33HNrAH4AfBRd98Vk78FfCHm+wLwr8D7sgrm7lcAVwAsXrzYm5ub8y5EMbS0tFCsedwz6x4aqhs4ovmIokxvLCtmvUoP1WtpqF5LQ/VaGqrX0lC9jj+V1i1nIzA78X0WsClfHjOrAaYC2/ob18xqCYH9de7+w1wGd3/W3TvdvQv4DqFb0LhT31SvbjkiIiIio0ClBff3AvPNbK6Z1RFukF2RyrMCODN+PhW4w8MbmlYAy+LTdOYC84FVsT/+lcAj7v615ITMrCnx9R3AQ0VfolFAb6kVERERGR0qqltO7EN/HnArUA1c5e6rzexi4D53X0EI1K81s7WEFvtlcdzVZnYT8DDhCTnnununmR0HvAf4k5k9GGf1j+6+EviKmS0kdMtZD7x/xBZ2BOWCe3cnnOuIiIiISDlUVHAPEIPulam0CxOf9wGn5Rl3ObA8lXYX2f3xcff3DLe8Y0FdUx1de7vo3NVJzdSKW6VERERERo1K65YjJVB/SHjWvfrdi4iIiJSXgnsZNr2lVkRERGR0UHAvw6bgXkRERGR0UHAvw1bfFLrlKLgXERERKS8F9zJs1QdUUzWxSn3uRURERMpMwb0Mm5mFx2FuUsu9iIiISDkpuJei0IusRERERMpPwb0URX1TvbrliIiIiJSZgnspCrXci4iIiJSfgnspirqmOjp3ddK5p7PcRRERERGpWArupSj0rHsRERGR8lNwL0VRf0h41r363YuIiIiUj4J7KQq13IuIiIiUn4J7KQoF9yIiIiLlp+BeiqK2sRarNQX3IiIiImWk4F6Kwsyoe1Gd+tyLiIiIlJGCeymauqY62jep5V5ERESkXBTcS9HoRVYiIiIi5VWRwb2ZLTGzNWa21szOzxheb2Y3xuH3mNmcxLALYvoaMzspps02szvN7BEzW21mH0nkn2Fmt5nZ4/H/9JFYxnKob6pXtxwRERGRMqq44N7MqoHLgJOBBcAZZrYgle1sYLu7zwMuBS6J4y4AlgFHAEuAy+P0OoBPuPsrgGOAcxPTPB+43d3nA7fH7+NSXVMdHVs76GrvKndRRERERCpSxQX3wFHAWnd/wt3bgRuApak8S4Fr4uebgRPMzGL6De7e5u7rgLXAUe6+2d1/D+DuLwCPAIdmTOsa4O0lWq6y634c5jPqmiMiIiJSDjXlLkAZHApsSHzfCBydL4+7d5jZTqAxpt+dGvfQ5IixC88i4J6YNNPdN8dpbTazg7MKZWbnAOcAzJw5k5aWlkEu1uC0trYWfx7Ph393/+TucE2kApWkXkX1WiKq19JQvZaG6rU0VK/jTyUG95aR5gXm6XdcM2sAfgB81N13DaZQ7n4FcAXA4sWLvbm5eTCjD1pLSwvFnscLU17g/n+8nyMOOYKDmg8q6rTHilLUq6heS0X1Whqq19JQvZaG6nX8qcRuORuB2Ynvs4BN+fKYWQ0wFdjW37hmVksI7K9z9x8m8jxrZk0xTxOwpWhLMsrUHaK31IqIiIiUUyUG9/cC881srpnVEW6QXZHKswI4M34+FbjD3T2mL4tP05kLzAdWxf74VwKPuPvX+pnWmcAtRV+iUaLu4DqoUnAvIiIiUi4V1y0n9qE/D7gVqAaucvfVZnYxcJ+7ryAE6tea2VpCi/2yOO5qM7sJeJjwhJxz3b3TzI4D3gP8ycwejLP6R3dfCXwZuMnMzgaeAk4buaUdWVZt1B2sZ92LiIiIlEvFBfcAMehemUq7MPF5H3mCcHdfDixPpd1Fdn983H0rcMIwizxm1DXV0bZJz7oXERERKYdK7JYjJaS31IqIiIiUj4J7KSoF9yIiIiLlo+Beiqq+qZ72Le14Z/rpoiIiIiJSagrupajqmuqgC9q3qPVeREREZKQpuJeiqmvSs+5FREREykXBvRSVgnsRERGR8lFwL0VV31QPQNtmPQ5TREREZKQpuJeiqnuRWu5FREREykXBvRRVVX0VNY01Cu5FREREykDBvRRdfVO9gnsRERGRMlBwL0VX11RH2yb1uRcREREZaQrupej0lloRERGR8lBwL0VX11RH+zPtuOsttSIiIiIjScG9FF19Uz2+39m/dX+5iyIiIiJSURTcS9HpRVYiIiIi5aHgXopOwb2IiIhIeSi4l6JTcC8iIiJSHgrupejqm+oBaNusx2GKiIiIjKSKC+7NbImZrTGztWZ2fsbwejO7MQ6/x8zmJIZdENPXmNlJifSrzGyLmT2UmtZFZva0mT0Y/04p5bKNFtWTq6k+oFot9yIiIiIjrKKCezOrBi4DTgYWAGeY2YJUtrOB7e4+D7gUuCSOuwBYBhwBLAEuj9MDuDqmZbnU3RfGv5XFXJ7RTM+6FxERERl5FRXcA0cBa939CXdvB24AlqbyLAWuiZ9vBk4wM4vpN7h7m7uvA9bG6eHuvwK2jcQCjBX1TfUK7kVERERGWE25CzDCDgU2JL5vBI7Ol8fdO8xsJ9AY0+9OjXtoAfM8z8zeC9wHfMLdt2dlMrNzgHMAZs6cSUtLSwGTHrrW1tbSzqMKeIKSL8doU/J6rVCq19JQvZaG6rU0VK+loXodfyotuLeMtPRrVPPlKWTctG8BX4j5vgD8K/C+rIzufgVwBcDixYu9ubl5gEkPT0tLC6Wcx9ofr2XT7zbx+je+nnDhozKUul4rleq1NFSvpaF6LQ3Va2moXsefSuuWsxGYnfg+C9iUL4+Z1QBTCV1uChm3F3d/1t073b0L+A6xG08lqGuqo2tvF527OstdFBEREZGKUWnB/b3AfDOba2Z1hBtkV6TyrADOjJ9PBe5wd4/py+LTdOYC84FV/c3MzJoSX98BPJQv73ijx2GKiIiIjLyK6pYT+9CfB9wKVANXuftqM7sYuM/dVwBXAtea2VpCi/2yOO5qM7sJeBjoAM51904AM/svoBk40Mw2Ap9z9yuBr5jZQkK3nPXA+0duacsr+SKryS+fXObSiIiIiFSGigruAeLjKFem0i5MfN4HnJZn3OXA8oz0M/Lkf8+wCjuG6S21IiIiIiOv0rrlyAiZcNgEqiZVse3nekKoiIiIyEhRcC8lUT2pmqazm9hy/Rbanla/exEREZGRoOBeSmbWx2bhnc7Gb2wsd1FEREREKoKCeymZiXMnctCpB7HpPzbRsauj3MURERERGfcU3EtJzf7kbDp3dbL5O5vLXRQRERGRcU/BvZTUAYsPYFrzNDZ+fSNd+7vKXRwRERGRcU3BvZTc7E/Opm1jG1tu2FLuooiIiIiMawrupeRmnDyDSUdMYsNXNxBe9isiIiIipaDgXkrOzJj9D7PZ/afdbP/59nIXR0RERGTcUnAvI2Lmu2ZSd0gdT331qXIXRURERGTcUnAvI6KqropZH57Fjtt38MIDL5S7OCIiIiLjkoJ7GTFN72+iuqGaDf+yodxFERERERmXFNzLiKmdVkvTOU1suXEL+57cV+7iiIiIiIw7Cu5lRM366CzMjI1f31juooiIiIiMOwruZURNmD2Bg5cdzKbvbGL/9v3lLo6IiIjIuKLgXkbc7H+YTdfuLjb9x6ZyF0VERERkXFFwLyOu4cgGpr9lOk9/42m62rrKXRwRERGRcUPBvZTF7E/Opv2Zdp79/rPlLoqIiIjIuKHgXspi+pun07CwgSe/9CStD7WWuzgiIiIi40JFBvdmtsTM1pjZWjM7P2N4vZndGIffY2ZzEsMuiOlrzOykRPpVZrbFzB5KTWuGmd1mZo/H/9NLuWxjhZnxkq+8hP1b9nPfq+7j4b95mD2P7Sl3sURERETGtIoL7s2sGrgMOBlYAJxhZgtS2c4Gtrv7POBS4JI47gJgGXAEsAS4PE4P4OqYlnY+cLu7zwduj98FmPGWGRyz7hhe/OkX8/z/PM+qBat49H2Psnf93nIXTURERGRMqrjgHjgKWOvuT7h7O3ADsDSVZylwTfx8M3CCmVlMv8Hd29x9HbA2Tg93/xWwLWN+yWldA7y9mAsz1tU21vKSL72EY544hlkfmsWz1z/LqsNX8djfP8a+jXrRlYiIiMhg1JS7AGVwKLAh8X0jcHS+PO7eYWY7gcaYfndq3EMHmN9Md98cp7XZzA7OymRm5wDnAMycOZOWlpaCFmaoWltbSz6PQVsKHAt8HzZ9dxObrtwEbwPewcC1PEqMynodB1SvpaF6LQ3Va2moXktD9Tr+VGJwbxlpXmCeQsYdEne/ArgCYPHixd7c3FyMyebV0tJCqecxZKfB3vV7efILT/LMNc/AD2DC3AlMP3E6M94yg2nHT6N2em25S5lpVNfrGKZ6LQ3Va2moXktD9VoaqtfxpxKD+43A7MT3WUD6bUq5PBvNrAaYSuhyU8i4ac+aWVNstW8Ctgyn8JVi4pyJvPzKl3PYZw9j60+2sv3n29ly3RY2f3szVMGUxVOY/pYQ7B/w2gOoqqvEHmYiIiIivVVicH8vMN/M5gJPE26QfVcqzwrgTOB3wKnAHe7uZrYCuN7MvgYcAswHVg0wv9y0vhz/31KsBakEE+dMZNZ5s5h13iy69nex655dbL9tO9tv285TX36Kp5Y/RdXEKia/cnKfv7qmOsKtEiIiIiKVoeKC+9iH/jzgVqAauMrdV5vZxcB97r4CuBK41szWElrsl8VxV5vZTcDDQAdwrrt3ApjZfwHNwIFmthH4nLtfSQjqbzKzs4GngNNGcHHHlaraKqYdN41px01j7ufn0rGzg+13bmfnL3fS+qdWtq7cyjPfe6Y7f830mu5Af9LLJlF/WD0TDpvAhBdPoGZGjQJ/ERERGXcqLrgHcPeVwMpU2oWJz/vIE4S7+3JgeUb6GXnybwVOGE55JVvN1BoOevtBHPT2g7rT2p9rZ/fq3ex+qOfv2eufpXNnZ69xqyZXdQf6uaC//pB66g6po66pjvqmep0AiIiIyJhTkcG9jF91B9VR11zH9Oaed4W5O/uf30/bU23se3Jf91/u+657d9GxtaPPtKzOugP9uqY66mbWUTO9hpppqb+pPZ9pH8mlFREREelNwb2Me2YWgv6D6pjy6imZeTp3d9K2uY32ze20b2rv83nPmj3s+NUOOnd24h39PyDpV5N+Rc30Gmqn14aTgRmJz9NrqJ5STc2UGqobqqmeEv/i51x61aQqXTUQERGRQVNwLwJUT65m0rxJTJo3qd987k7Xni46dnSEv50dPZ+3d/D4A49zyLRD6Ngevu/ftp996/bR+vtWOrZ30Nna2e/0k6omV1E9OQb+k6u7P1dNrqKqvgqrNapqq7A66/u5vve4VZOreqaTS4vTsJowTvfnatOJhYiIyBil4F5kEMysO9CuP7S+z/DHWx5nXvO8vON37e+is7Uz/L0Q/1o76Xiho/tz5wuddO6Of62ddO2O48Tv7Vva6Wrrwvc7vt/pau/7ma5hLmcM9qsmVFE9qZqqiVXdf9UTE9/rq3pOCGp6/qim+3PupKOqLp5MJD53pyWm0T1udU86q2Fn/c5+CkwoV+6EaFI8CarRI1JFRKSyKLgXGUFVtVVUTa8q+Uu4ujq6ep8UpE8Wdnf2OkHwDqdrf+p7exdde3v+Ovd20rUvfN7//P6Q3taFd4b83uG9PtNJ9zSHe7IB8AAPDHocqw0nY1WTqrrfheCe6FaV6mFlVflPMJInL72ueOROYOJ3Ms4nel0JMfpMIzm+1cYTGot5redz9/eqMI1eJ0zpk6g4HatOLU8ijWdh34Z93a/ny5qXu4d6in/p71SRfRWpWld/RETKQcG9yDhUVVNF1dQqaqaOjk3cO+PJQ3viCkN74qpDxgkCnZ7c8KoAAAfxSURBVHSn/fEPf+RVR74q//S7PJyA7O6ka09X98lM9+c9nXi797xjOhlrx8DbPZyEdJclXY5O77lCsrcrLE9Hz8lQ94mRp84Y0rdodNFzMpUYrzjvuh68u7m7NBNOBP1UJ05KquIJRPKz0V33dIXfs/u3iJ+tJnQ3s/pwRamqPvE3IZ5YVMd5VFuYdnU8YYvpGH1PVEikEU+C6vJ3ebNqC+VMrhedvdcXNsFjNz0WypI4YUoub696sNTnqp4TzXx/VNOzzu/u6nUSn/vu+52qSYmrb4nPuf/dy1rbc3LZ53vySlvGVTig17bQZ9vo9AGv4uXqNV/d0gk803MymqunPvWaO5FOb+u55KqME94BuiF6V6ocufWkNq5nIqPM6Djyi8i4ZtVGdXU1TBjiBOphRvOMopZptPGuRDDU4f22mHtX4oSivefEqftkKabRSWbwmfu+5uE1HP6yw2MBsufV7xUES5Q7Txcx398ToOfKTlfiZKorzKtPYJ4L/uOJgXc4XW3halHXvnjVqM27v3fu7Ox9gpA7MejsCc5y0svSnUbixKu993J0tXd1B3ZAT1kzroywH56rea57GXvVaVfG52Se+HmwqiZU9XRLi39WY+x/fj+de+KJ7t7wv2tvES6llUlJTkaNXt0Cu7eRLu/9m2dJXrlKXH3rPpHM8dT/3LjJdT35v9qy15v0upKaZq8TVqBuZh2Lf794SNUiY5eCexGRUcCqDKs36HsrR8msaVnDIc2HjNwMxzjvCgHfQK29LS0tvK75dcOfV6qrW/qvamLPPSaD6QblXeGkqHN3uKKVO1Hs1TUv9b3P1bb4uautC7Ps7mrd/2vCyVmfK3eJk1I6CSdJya5wuROn+HnNo2t42cte1n1C2H2ymPgfFrD3/14Bb+7EL88Jr3d679b9qr7lAPrWWUfvestJd8nr/p8sf1fv/91XCAq5ypNxNTJ5wlo9tbrg9ULGDwX3IiIiBbCqkeuGYVWh20qppl09sZrqiWMr8FvTsoam5qZyF0Nk1NOjJERERERExgkF9yIiIiIi44SCexERERGRcULBvYiIiIjIOKHgXkRERERknFBwLyIiIiIyTii4FxEREREZJxTci4iIiIiME9b95jYZNczsOeDJEs/mQOD5Es+jEqleS0P1Whqq19JQvZaG6rU0ilGvh7n7QcUojAyfgvsKZWb3ufvicpdjvFG9lobqtTRUr6Whei0N1WtpqF7HH3XLEREREREZJxTci4iIiIiMEwruK9cV5S7AOKV6LQ3Va2moXktD9VoaqtfSUL2OM+pzLyIiIiIyTqjlXkRERERknFBwLyIiIiIyTii4rzBmtsTM1pjZWjM7v9zlGcvM7Coz22JmDyXSZpjZbWb2ePw/vZxlHGvMbLaZ3Wlmj5jZajP7SExXvQ6DmU0ws1Vm9odYr5+P6XPN7J5YrzeaWV25yzoWmVm1mT1gZj+J31Wvw2Rm683sT2b2oJndF9O0HxgmM5tmZjeb2aNxP/ta1ev4o+C+gphZNXAZcDKwADjDzBaUt1Rj2tXAklTa+cDt7j4fuD1+l8J1AJ9w91cAxwDnxnVU9To8bcDx7n4ksBBYYmbHAJcAl8Z63Q6cXcYyjmUfAR5JfFe9Fseb3H1h4hns2g8M378BP3P3lwNHEtZb1es4o+C+shwF/P/27ifEyioO4/j3wVEIK0RLCacQIcSNqFtFRMNFSrpICBSkjRs3LUSwTRC4FXdu1HDhH0TTXBZY1EpCC5LapEQO/hkhpHSRVE+Lc8RBpsXMO/p2z30+MNxzzn0XP57Fmd+897x3frZ9w/Yj4DSwteeaBpbtr4HfnlreChyv4+PAtuda1ICzfdv21Tr+g/KLZzHJtRMXD+p0dv0xsAE4W9eT6zRIGgU2A0fqXCTXZyX7QAeSXgbWAUcBbD+yfZ/k2pw098NlMXBzwnysrsXMWWT7NpRGFVjYcz0DS9ISYBVwmeTaWT068j0wDnwBXAfu2/6rXpL9YHoOAfuAf+p8Acl1Jhj4XNIVSbvrWvaBbpYC94BP6jGyI5Lmklybk+Z+uGiStXwXavzvSHoROAd8YPv3vutpge2/ba8ERimf4i2f7LLnW9Vgk7QFGLd9ZeLyJJcm16lbY3s15RjpHknr+i6oASPAauCw7VXAQ3IEp0lp7ofLGPD6hPkocKunWlp1V9JrAPV1vOd6Bo6k2ZTG/oTtT+tycp0h9WP4ryjPNMyTNFLfyn4wdWuAdyT9QjnmuIFyJz+5dmT7Vn0dB85T/iDNPtDNGDBm+3Kdn6U0+8m1MWnuh8u3wJv1mxzmAO8BF3uuqTUXgV11vAv4rMdaBk49r3wU+Mn2wQlvJdcOJL0qaV4dvwC8RXme4Uvg3XpZcp0i2/ttj9peQtlPL9neQXLtRNJcSS89HgObgGtkH+jE9h3gpqRldWkj8CPJtTn5D7VDRtLblDtLs4Bjtg/0XNLAknQKWA+8AtwFPgIuAGeAN4Bfge22n37oNv6DpLXAN8APPDnD/CHl3H1ynSZJKygPys2i3NQ5Y/tjSUspd5znA98BO23/2V+lg0vSemCv7S3JtZua3/k6HQFO2j4gaQHZBzqRtJLy8Pcc4AbwPnVPILk2I819REREREQjciwnIiIiIqIRae4jIiIiIhqR5j4iIiIiohFp7iMiIiIiGpHmPiIiIiKiEWnuIyIiIiIakeY+IiIiIqIR/wLmsozHlxcJngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_relu_time = time()\n",
    "\n",
    "# Try different MLP models\n",
    "nn_relu = MLP(input_data.shape[1], [128,128,128], label_data.shape[1], 'relu', weight_norm=True, dropout=True, \\\n",
    "              keep_prob=0.99, output_softmax_crossEntropyLoss=True, weight_decay=True, weight_lambda=0.000005)\n",
    "\n",
    "# Try different fit models\n",
    "MSE_relu = nn_relu.fit(trainDat, trainLabel, learning_rate=0.0024, epochs=60, gd='mini_batch', \\\n",
    "                       momentum=True, gamma_MT=0.9, mini_batch_size=512, batch_norm=False)\n",
    "\n",
    "print(\"------------------\\n\")\n",
    "print('last loss:%f' % MSE_relu[-1])\n",
    "\n",
    "end_relu_fit_time = time()\n",
    "print(\"--- Relu Fit used: %2f seconds\" % (end_relu_fit_time - start_relu_time))\n",
    "\n",
    "pl.figure(figsize=(10,4))\n",
    "pl.title('relu function with softmax output layer\\'s cross entropy loss in each epoch', fontsize=20) \n",
    "pl.plot(MSE_relu, color=\"m\")\n",
    "pl.grid()\n",
    "pl.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- the accuracy is: 85.5000%\n"
     ]
    }
   ],
   "source": [
    "# Predict splited test data\n",
    "predictLabel_relu = predict_data(testDat, nn_relu)\n",
    "acc_relu = calculateAcc(predictLabel_relu, realLabel)\n",
    "# build_confusion_matrix(predictLabel_relu, realLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dropout\n",
      "Using dropout\n",
      "Using softmax and cross entropy loss in output\n",
      "Using batch normalization\n",
      "Using momentum\n",
      "Using mini batch\n",
      "the mini batch size is 512\n",
      "---------------------------------------\n",
      "--- the epoch 1 loss: 0.002839 and used: 1.026965 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 2 loss: 0.002308 and used: 1.012665 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 3 loss: 0.002188 and used: 0.947351 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 4 loss: 0.002120 and used: 1.046582 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 5 loss: 0.002069 and used: 1.015858 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 6 loss: 0.002035 and used: 0.974100 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 7 loss: 0.002006 and used: 1.101889 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 8 loss: 0.001985 and used: 1.110254 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 9 loss: 0.001964 and used: 1.027254 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 10 loss: 0.001947 and used: 1.041455 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 11 loss: 0.001931 and used: 1.204478 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 12 loss: 0.001920 and used: 1.161045 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 13 loss: 0.001907 and used: 1.282185 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 14 loss: 0.001898 and used: 1.197514 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 15 loss: 0.001890 and used: 0.995132 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 16 loss: 0.001883 and used: 1.014233 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 17 loss: 0.001875 and used: 1.224823 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 18 loss: 0.001868 and used: 1.031426 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 19 loss: 0.001862 and used: 1.111813 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 20 loss: 0.001855 and used: 1.276186 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 21 loss: 0.001852 and used: 1.273583 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 22 loss: 0.001845 and used: 1.137423 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 23 loss: 0.001839 and used: 1.151681 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 24 loss: 0.001834 and used: 1.128605 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 25 loss: 0.001828 and used: 1.210475 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 26 loss: 0.001822 and used: 1.092949 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 27 loss: 0.001821 and used: 1.104674 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 28 loss: 0.001815 and used: 1.180080 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 29 loss: 0.001809 and used: 1.096313 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 30 loss: 0.001807 and used: 1.137096 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 31 loss: 0.001802 and used: 1.059007 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 32 loss: 0.001803 and used: 1.146661 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 33 loss: 0.001796 and used: 1.318880 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 34 loss: 0.001794 and used: 1.129191 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 35 loss: 0.001792 and used: 1.166982 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 36 loss: 0.001785 and used: 1.281547 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 37 loss: 0.001787 and used: 1.328004 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 38 loss: 0.001781 and used: 1.178033 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 39 loss: 0.001777 and used: 1.188503 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 40 loss: 0.001779 and used: 1.298366 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 41 loss: 0.001776 and used: 1.229522 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 42 loss: 0.001775 and used: 1.151891 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 43 loss: 0.001770 and used: 1.210239 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 44 loss: 0.001767 and used: 1.112909 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 45 loss: 0.001764 and used: 1.121317 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 46 loss: 0.001761 and used: 1.157163 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 47 loss: 0.001758 and used: 1.184578 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 48 loss: 0.001758 and used: 1.266468 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 49 loss: 0.001754 and used: 1.055413 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 50 loss: 0.001752 and used: 1.017305 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 51 loss: 0.001752 and used: 1.041006 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 52 loss: 0.001750 and used: 1.038709 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 53 loss: 0.001747 and used: 1.179103 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 54 loss: 0.001744 and used: 0.991160 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 55 loss: 0.001742 and used: 1.103674 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 56 loss: 0.001740 and used: 1.003339 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 57 loss: 0.001741 and used: 0.985449 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 58 loss: 0.001739 and used: 1.028889 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 59 loss: 0.001738 and used: 0.985137 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 60 loss: 0.001736 and used: 0.985168 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 61 loss: 0.001732 and used: 0.969979 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 62 loss: 0.001733 and used: 1.080191 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 63 loss: 0.001730 and used: 0.980261 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 64 loss: 0.001730 and used: 0.963814 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 65 loss: 0.001729 and used: 0.965575 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 66 loss: 0.001726 and used: 0.994146 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 67 loss: 0.001724 and used: 0.999801 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 68 loss: 0.001723 and used: 1.103839 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 69 loss: 0.001720 and used: 0.988462 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 70 loss: 0.001720 and used: 1.121596 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 71 loss: 0.001719 and used: 1.166607 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 72 loss: 0.001717 and used: 1.117945 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 73 loss: 0.001716 and used: 1.090963 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 74 loss: 0.001715 and used: 1.094158 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 75 loss: 0.001709 and used: 0.986211 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 76 loss: 0.001707 and used: 1.086933 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 77 loss: 0.001706 and used: 1.085907 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 78 loss: 0.001705 and used: 1.108557 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 79 loss: 0.001707 and used: 1.118411 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 80 loss: 0.001705 and used: 1.290081 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 81 loss: 0.001706 and used: 1.281316 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 82 loss: 0.001704 and used: 1.149597 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "--- the epoch 83 loss: 0.001699 and used: 1.251980 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 84 loss: 0.001700 and used: 1.235116 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 85 loss: 0.001699 and used: 1.262945 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 86 loss: 0.001698 and used: 1.290392 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 87 loss: 0.001696 and used: 1.117949 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 88 loss: 0.001697 and used: 1.059070 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 89 loss: 0.001693 and used: 0.991398 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 90 loss: 0.001697 and used: 1.084697 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 91 loss: 0.001694 and used: 1.058772 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 92 loss: 0.001692 and used: 1.091727 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 93 loss: 0.001691 and used: 1.065921 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 94 loss: 0.001693 and used: 1.076780 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 95 loss: 0.001692 and used: 1.061262 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 96 loss: 0.001690 and used: 0.970680 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 97 loss: 0.001688 and used: 0.951111 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 98 loss: 0.001688 and used: 0.950460 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 99 loss: 0.001690 and used: 0.986470 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 100 loss: 0.001689 and used: 0.970705 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 101 loss: 0.001686 and used: 1.004023 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 102 loss: 0.001684 and used: 1.226844 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 103 loss: 0.001684 and used: 1.118829 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 104 loss: 0.001686 and used: 1.237916 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 105 loss: 0.001683 and used: 1.296421 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 106 loss: 0.001682 and used: 1.191740 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 107 loss: 0.001682 and used: 1.042670 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 108 loss: 0.001682 and used: 0.959162 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 109 loss: 0.001681 and used: 1.041778 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 110 loss: 0.001680 and used: 0.945497 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 111 loss: 0.001682 and used: 1.022212 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 112 loss: 0.001677 and used: 1.032840 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 113 loss: 0.001680 and used: 1.229165 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 114 loss: 0.001678 and used: 1.108631 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 115 loss: 0.001675 and used: 1.099301 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 116 loss: 0.001678 and used: 1.211393 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 117 loss: 0.001675 and used: 1.138292 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 118 loss: 0.001673 and used: 1.133265 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 119 loss: 0.001673 and used: 1.213583 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 120 loss: 0.001674 and used: 1.104025 seconds\n",
      "------------------\n",
      "\n",
      "last loss:0.001674\n",
      "--- Tanh Fit used: 132.887579 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_tanh_time = time()\n",
    "\n",
    "# Try different MLP models\n",
    "nn_tanh = MLP(input_data.shape[1], [128,256,128], label_data.shape[1], 'tanh', weight_norm=False, dropout=True, \\\n",
    "                      keep_prob=0.99, output_softmax_crossEntropyLoss=True, weight_decay=True, weight_lambda=0.0000004)\n",
    "# Try different fit models\n",
    "MSE_tanh = nn_tanh.fit(input_data, label_data, learning_rate=0.08, epochs=120, gd='mini_batch', \\\n",
    "                               momentum=True, gamma_MT=0.9, mini_batch_size=512, batch_norm=True)\n",
    "print(\"------------------\\n\")\n",
    "print('last loss:%f' % MSE_tanh[-1])\n",
    "\n",
    "end_tanh_fit_time = time()\n",
    "print(\"--- Tanh Fit used: %2f seconds\" % (end_tanh_fit_time - start_tanh_time))\n",
    "\n",
    "pl.figure(figsize=(10,4))\n",
    "pl.title('Tanh function with softmax output layer\\'s cross entropy loss in each epoch', fontsize=20) \n",
    "pl.plot(MSE_tanh, color=\"r\")\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- the accuracy is: 97.9500%\n"
     ]
    }
   ],
   "source": [
    "predictLabel_tanh = predict_data(testDat, nn_tanh)\n",
    "acc_tanh = calculateAcc(predictLabel_tanh, realLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weight normalization\n",
      "Using dropout\n",
      "Using dropout\n",
      "Using softmax and cross entropy loss in output\n",
      "Using momentum\n",
      "Using mini batch\n",
      "the mini batch size is 512\n",
      "---------------------------------------\n",
      "--- the epoch 1 loss: 0.004394 and used: 0.760018 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 2 loss: 0.003741 and used: 0.738018 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 3 loss: 0.002042 and used: 0.668154 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 4 loss: 0.001104 and used: 0.714382 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 5 loss: 0.000997 and used: 0.679591 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 6 loss: 0.000949 and used: 0.715566 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 7 loss: 0.000918 and used: 0.687363 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 8 loss: 0.000898 and used: 0.725566 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 9 loss: 0.000884 and used: 0.689342 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 10 loss: 0.000871 and used: 0.671470 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 11 loss: 0.000859 and used: 0.725434 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 12 loss: 0.000851 and used: 0.688010 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 13 loss: 0.000846 and used: 0.675034 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 14 loss: 0.000838 and used: 0.674444 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 15 loss: 0.000834 and used: 0.711787 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 16 loss: 0.000832 and used: 0.696756 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 17 loss: 0.000824 and used: 0.727744 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 18 loss: 0.000819 and used: 0.754282 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 19 loss: 0.000821 and used: 0.682801 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 20 loss: 0.000813 and used: 0.686137 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 21 loss: 0.000811 and used: 0.699832 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 22 loss: 0.000809 and used: 0.730865 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 23 loss: 0.000805 and used: 0.759699 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 24 loss: 0.000801 and used: 0.701805 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 25 loss: 0.000801 and used: 0.695756 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 26 loss: 0.000800 and used: 0.693398 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 27 loss: 0.000797 and used: 0.698304 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 28 loss: 0.000792 and used: 0.728164 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 29 loss: 0.000790 and used: 0.708702 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 30 loss: 0.000791 and used: 0.752198 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 31 loss: 0.000792 and used: 0.744398 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 32 loss: 0.000791 and used: 0.692866 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 33 loss: 0.000786 and used: 0.726275 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 34 loss: 0.000785 and used: 0.707978 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 35 loss: 0.000783 and used: 0.720979 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 36 loss: 0.000783 and used: 0.703544 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 37 loss: 0.000782 and used: 0.728326 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 38 loss: 0.000779 and used: 0.777813 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 39 loss: 0.000777 and used: 0.737497 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 40 loss: 0.000781 and used: 0.716891 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 41 loss: 0.000777 and used: 0.758625 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 42 loss: 0.000776 and used: 0.731096 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 43 loss: 0.000771 and used: 0.697995 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 44 loss: 0.000775 and used: 0.713143 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 45 loss: 0.000774 and used: 0.707877 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 46 loss: 0.000774 and used: 0.747834 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 47 loss: 0.000773 and used: 0.761716 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 48 loss: 0.000772 and used: 0.721112 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 49 loss: 0.000771 and used: 0.726962 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 50 loss: 0.000770 and used: 0.745632 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 51 loss: 0.000769 and used: 0.750653 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 52 loss: 0.000767 and used: 0.743536 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 53 loss: 0.000768 and used: 0.777233 seconds\n",
      "---------------------------------------\n",
      "--- the epoch 54 loss: 0.000772 and used: 0.796953 seconds\n"
     ]
    }
   ],
   "source": [
    "start_result_time = time()\n",
    "\n",
    "\n",
    "def predictResult(nn = 'nn_2'):\n",
    "    if nn == 'nn_1':\n",
    "        nn = MLP(input_data.shape[1], [128,128,128], label_data.shape[1], 'relu', weight_norm=True, dropout=True, \\\n",
    "                      keep_prob=0.99, output_softmax_crossEntropyLoss=True, weight_decay=True, weight_lambda=0.000005)\n",
    "        MSE = nn.fit(input_data, label_data, learning_rate=0.0024, epochs=60, gd='mini_batch', \\\n",
    "                               momentum=True, gamma_MT=0.9, mini_batch_size=512, batch_norm=False)\n",
    "    if nn == 'nn_2':\n",
    "        nn = MLP(input_data.shape[1], [128,256,128], label_data.shape[1], 'tanh', weight_norm=False, dropout=False, \\\n",
    "                      keep_prob=0.5, output_softmax_crossEntropyLoss=True, weight_decay=True, weight_lambda=0.0000004)        \n",
    "        MSE = nn.fit(input_data, label_data, learning_rate=0.08, epochs=120, gd='mini_batch', \\\n",
    "                               momentum=True, gamma_MT=0.9, mini_batch_size=512, batch_norm=True)\n",
    "    return nn, MSE\n",
    "\n",
    "\n",

    "nn, MSE = predictResult(nn = 'nn_1')\n",
    "\n",
    "\n",
    "print(\"------------------\\n\")\n",
    "print('last loss:%f' % MSE[-1])\n",
    "\n",
    "end_result_fit_time = time()\n",
    "print(\"--- The model Fit used: %2f seconds\" % (end_result_fit_time - start_result_time))\n",
    "\n",
    "pl.figure(figsize=(10,4))\n",
    "pl.title('the function loss in each epoch') \n",
    "pl.plot(MSE, color=\"r\")\n",
    "pl.grid()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The model predict used: 0.076156 seconds\n"
     ]
    }
   ],
   "source": [
    "start_predict_time = time()\n",
    "\n",
    "predictLabel = predict_data(test_data, nn)\n",
    "\n",
    "end_predict_time = time()\n",
    "print(\"--- The model predict used: %2f seconds\" % (end_predict_time - start_predict_time))\n",
    "\n",
    "output_data = pd.DataFrame(predictLabel)\n",
    "output_data.to_csv(\"Predicted_labels.h5\", sep=\",\", index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
